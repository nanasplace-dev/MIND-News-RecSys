{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4779a3c6-49e0-4c04-a51c-b52f424adb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# å…³é”®ï¼šè®¾ç½® Hugging Face å›½å†…é•œåƒç«™\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# --- ç„¶åæ‰æ˜¯åŸæœ¬çš„ä»£ç  ---\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002a1072-11cf-4346-8d62-eec9760d77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr_torch.models import DIN, DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7697857b-1549-4808-9cb6-59c3c69b0ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting seaborn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from seaborn) (2.3.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from seaborn) (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# ä½¿ç”¨æ¸…åé•œåƒæºå¿«é€Ÿå®‰è£…\n",
    "!{sys.executable} -m pip install seaborn -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8a15cb-236d-4912-98dc-8f1a457c234a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-rechub\n",
      "  Downloading torch_rechub-0.0.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch-rechub) (2.2.6)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch-rechub) (2.9.1)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch-rechub) (2.3.3)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch-rechub) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch-rechub) (1.7.2)\n",
      "Collecting annoy>=1.17.0 (from torch-rechub)\n",
      "  Downloading annoy-1.17.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.0.5->torch-rechub) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.0.5->torch-rechub) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.0.5->torch-rechub) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->torch-rechub) (1.17.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from scikit-learn>=0.23.2->torch-rechub) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from scikit-learn>=0.23.2->torch-rechub) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from scikit-learn>=0.23.2->torch-rechub) (3.6.0)\n",
      "Requirement already satisfied: filelock in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from torch>=1.7.0->torch-rechub) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.7.0->torch-rechub) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->torch-rechub) (3.0.3)\n",
      "Downloading torch_rechub-0.0.3-py3-none-any.whl (78 kB)\n",
      "Downloading annoy-1.17.3-cp310-cp310-macosx_11_0_arm64.whl (57 kB)\n",
      "Installing collected packages: annoy, torch-rechub\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [torch-rechub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annoy-1.17.3 torch-rechub-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-rechub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8037ff-c1e2-4937-b66f-2a6487f93563",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymilvus\n",
      "  Downloading pymilvus-2.6.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: setuptools>69 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pymilvus) (80.9.0)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pymilvus) (1.76.0)\n",
      "Collecting orjson>=3.10.15 (from pymilvus)\n",
      "  Downloading orjson-3.11.4-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pymilvus) (6.33.1)\n",
      "Collecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pymilvus) (2.3.3)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2->pymilvus) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n",
      "Downloading pymilvus-2.6.3-py3-none-any.whl (273 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading orjson-3.11.4-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (243 kB)\n",
      "Installing collected packages: python-dotenv, orjson, pymilvus\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [pymilvus]\n",
      "\u001b[1A\u001b[2KSuccessfully installed orjson-3.11.4 pymilvus-2.6.3 python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8c6d50-c68b-4c00-890f-250d3a94e92e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: pillow in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/jiaxiwang/Desktop/anaconda/miniconda3/envs/recsys_py310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8949d829-ad05-4eea-a300-cb880c0f0bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥ GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1e19f-7075-4e31-832a-656e064f553b",
   "metadata": {},
   "source": [
    "# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (MIND)â€”â€”æ•°æ®æ¸…æ´—ä¸ç‰¹å¾å·¥ç¨‹\n",
    "\n",
    "#### ä¸¤å¼ è¡¨ï¼šnewè¡¨ã€behaviorsè¡¨ï¼ˆè°åœ¨ä»€ä¹ˆèƒŒæ™¯ä¸‹åšäº†é€‰æ‹©ï¼‰\n",
    "##### å”¯ä¸€æ ‡è¯† (news_id)\n",
    "\n",
    "##### behaviorsè¡¨ï¼šä¸Šä¸‹æ–‡/é•¿æœŸå…´è¶£ (history)ï¼š\n",
    "ä¸€ä¸²ç©ºæ ¼åˆ†éš”çš„ news_idï¼ˆå¦‚ N8668 N39081...ï¼‰ï¼Œè¿™æ˜¯ç”¨æˆ·åœ¨è¿™æ¬¡æ›å…‰ä¹‹å‰ç‚¹å‡»è¿‡çš„æ‰€æœ‰æ–°é—»ã€‚è¿™æ˜¯æ¨¡å‹ç”¨æ¥å­¦ä¹ â€œç”¨æˆ·å–œæ¬¢ä»€ä¹ˆâ€çš„ä¸»è¦ä¾æ®ã€‚\n",
    "##### behaviorsè¡¨ï¼š é¢„æµ‹ç›®æ ‡/å½“å‰å€™é€‰é›† (impressions)ï¼š\n",
    "æ˜¯è¿™ç§æ ¼å¼ï¼šN123-1 N456-0 N789-0 ...ï¼ˆæ–°é—»ID-ç‚¹å‡»æ ‡ç­¾ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒ â€œç‚¸å¼€â€ (Explode)ï¼Œå˜æˆä¸€è¡Œä¸€æ¡æ ·æœ¬ï¼š(User, History, Target_News, Label)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de7185e-e77c-4b9b-8b51-54b01a70e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®åŠ è½½å®Œæˆï¼\n",
      "æ€»æ–°é—»æ•°: 104151\n",
      "è®­ç»ƒè¡Œä¸ºæ•°: 2232748\n",
      "éªŒè¯è¡Œä¸ºæ•°: 376471\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰ MIND æ•°æ®é›†çš„åˆ—å\n",
    "news_cols = ['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities']\n",
    "behaviors_cols = ['impression_id', 'user_id', 'time', 'history', 'impressions']\n",
    "# è¯»å–è®­ç»ƒæ•°æ®\n",
    "train_news = pd.read_csv('data/MINDlarge_train/news.tsv', sep='\\t', header=None, names=news_cols)\n",
    "train_behaviors = pd.read_csv('data/MINDlarge_train/behaviors.tsv', sep='\\t', header=None, names=behaviors_cols)\n",
    "# è¯»å–éªŒè¯æ•°æ®\n",
    "valid_news = pd.read_csv('data/MINDlarge_dev/news.tsv', sep='\\t', header=None, names=news_cols)\n",
    "valid_behaviors = pd.read_csv('data/MINDlarge_dev/behaviors.tsv', sep='\\t', header=None, names=behaviors_cols)\n",
    "# åˆå¹¶æ–°é—»æ•°æ®ï¼ˆdrop_duplicateså»é‡ï¼‰\n",
    "all_news = pd.concat([train_news, valid_news]).drop_duplicates(subset=['news_id'])\n",
    "\n",
    "print(\"æ•°æ®åŠ è½½å®Œæˆï¼\")\n",
    "print(f\"æ€»æ–°é—»æ•°: {len(all_news)}\")\n",
    "print(f\"è®­ç»ƒè¡Œä¸ºæ•°: {len(train_behaviors)}\")\n",
    "print(f\"éªŒè¯è¡Œä¸ºæ•°: {len(valid_behaviors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91b0bb4-ec93-43e6-99fb-a90fd2f76737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_newsè¡¨\n",
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "  news_id   category               subcategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             url  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                      title_entities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                   abstract_entities  \n",
      "0                                                 []  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
      "3                                                 []  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
      "train_behaviorsè¡¨\n",
      "   impression_id  user_id                    time  \\\n",
      "0              1   U87243  11/10/2019 11:30:54 AM   \n",
      "1              2  U598644   11/12/2019 1:45:29 PM   \n",
      "2              3  U532401  11/13/2019 11:23:03 AM   \n",
      "3              4  U593596  11/12/2019 12:24:09 PM   \n",
      "4              5  U239687   11/14/2019 8:03:01 PM   \n",
      "\n",
      "                                             history  \\\n",
      "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
      "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
      "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
      "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
      "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
      "\n",
      "                                         impressions  \n",
      "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
      "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
      "2              N103852-0 N53474-0 N127836-0 N47925-1  \n",
      "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...  \n",
      "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...  \n",
      "all_newsè¡¨\n",
      "  news_id   category               subcategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N86255     health                   medical   \n",
      "4  N93187       news                 newsworld   \n",
      "\n",
      "                                               title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  Dispose of unwanted prescription drugs during ...   \n",
      "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3                                                NaN   \n",
      "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "\n",
      "                                             url  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
      "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "\n",
      "                                      title_entities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
      "4                                                 []   \n",
      "\n",
      "                                   abstract_entities  cat_idx  subcat_idx  \n",
      "0                                                 []        8         151  \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...       12         197  \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...        6         281  \n",
      "3                                                 []        6         161  \n",
      "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...       12         205  \n"
     ]
    }
   ],
   "source": [
    "print(\"train_newsè¡¨\")\n",
    "print(train_news.head(5))\n",
    "print(\"train_behaviorsè¡¨\")\n",
    "print(train_behaviors.head(5))\n",
    "print(\"all_newsè¡¨\")\n",
    "print(all_news.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529b78d-93df-4c7a-893b-c862334af874",
   "metadata": {},
   "source": [
    "### ç¬¬ 1 æ­¥ï¼šæ•°æ®æ¸…æ´—ä¸æ ·æœ¬æ„å»ºâ€”â€”è´Ÿé‡‡æ ·å’Œæ ·æœ¬å±•å¹³\n",
    "\n",
    "è¿™ä¸€æ­¥æˆ‘ä»¬å°†æŠŠâ€œä¸€æ¬¡æ›å…‰ï¼ˆSessionï¼‰â€æ‹†è§£æˆâ€œå¤šä¸ªè®­ç»ƒæ ·æœ¬â€ã€‚ä¸ºäº†é˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä¼šåšä¸€ä¸ª è´Ÿé‡‡æ · (Negative Sampling)ï¼šå¯¹äºæ¯ä¸ªç‚¹å‡»ï¼ˆæ­£æ ·æœ¬ï¼‰ï¼Œæˆ‘ä»¬éšæœºé€‰ 4 ä¸ªæ²¡ç‚¹å‡»çš„ï¼ˆè´Ÿæ ·æœ¬ï¼‰ã€‚\n",
    "\n",
    "#### è¾“å…¥ï¼šåŸå§‹ behaviors è¡¨ï¼Œè¾“å‡ºï¼šä¸€ä¸ªæ–°çš„ DataFrame\n",
    "å°†åŸå§‹behaviorsè¡¨çš„impressionsâ€œä¸€ä¸ªæ ·æœ¬(ä¸€è¡Œ)ä»£è¡¨ä¸€æ¬¡æ›å…‰(åŒ…å«å¤šä¸ªæ–°é—»)â€æ ¼å¼è½¬æ¢ä¸ºâ€œä¸€ä¸ªæ ·æœ¬(ä¸€è¡Œ)ä»£è¡¨ä¸€å¯¹User-Itemå…³ç³»â€ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé›†ä¸­å¼ºè¡Œæ§åˆ¶æ­£è´Ÿæ ·æœ¬çš„æ¯”ä¾‹ã€‚\n",
    "\n",
    "##### è®­ç»ƒé›†ç­–ç•¥ï¼šæ‰©å……æ­£æ ·æœ¬ï¼Œéšæœºé‡‡æ ·è´Ÿæ ·æœ¬ã€‚å¼ºåˆ¶å®ç° 1 ä¸ªæ­£æ ·æœ¬æ­é… neg_ratio (4) ä¸ªè´Ÿæ ·æœ¬ã€‚\n",
    "##### éªŒè¯é›†ç­–ç•¥ï¼šå…¨é‡ä¿ç•™ã€‚ä¸ä¸¢å¼ƒä»»ä½•æ•°æ®ï¼Œè¿˜åŸçœŸå®åœºæ™¯ç”¨äºè¯„ä¼°ã€‚\n",
    "éªŒè¯é›†æ¨¡æ‹ŸçœŸå®ä¸–ç•Œã€‚åœ¨çœŸå®ä¸–ç•Œæ¨èä¸­ï¼Œæˆ‘ä»¬ä¸èƒ½æ•…æ„æŠŠè´Ÿæ ·æœ¬æ‰”æ‰ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°æ¨¡å‹åœ¨é¢å¯¹å¤§é‡è´Ÿæ ·æœ¬æ—¶ï¼Œèƒ½å¦ä¾ç„¶æŠŠæ­£æ ·æœ¬æ’åœ¨å‰é¢ã€‚å› æ­¤ï¼Œæ‰€æœ‰æ›å…‰æ•°æ®éƒ½è¦ä¿ç•™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be4e2f7-0739-4aa0-813c-dd10f2de2592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨é‡æ–°å¤„ç†æ•°æ® (1:4 é‡‡æ ·)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2232748/2232748 [00:52<00:00, 42182.18it/s]\n",
      "Processing valid: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 376471/376471 [00:17<00:00, 21518.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æœ€ç»ˆè®­ç»ƒé›†å¤§å°: 16918280\n",
      "è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ (åº”æ¥è¿‘ 1:4):\n",
      "label\n",
      "0    13534624\n",
      "1     3383656\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_mind_data_ratio(df, mode='train', neg_ratio=4):\n",
    "    \"\"\"\n",
    "    å‡çº§ç‰ˆæ•°æ®å¤„ç†ï¼š\n",
    "    1. ä¿æŒæ­£è´Ÿæ ·æœ¬ 1:N=neg_ratio çš„æ¯”ä¾‹ (é€šå¸¸ 1:4 æ•ˆæœæœ€å¥½)\n",
    "    2. è®©æ¨¡å‹è§è¯†æ›´å¤šè´Ÿæ ·æœ¬ï¼Œæå‡åˆ¤åˆ«èƒ½åŠ›\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Processing {mode}\"):\n",
    "        user_id = str(row['user_id'])    \n",
    "        # å¤„ç†å†å²è®°å½•historyï¼šå¦‚æœæ˜¯ç¼ºå¤±å€¼ NaNï¼ˆç©ºå€¼ï¼‰ï¼Œå¡«è¡¥ä¸º 'N0'ï¼ˆé˜²æ­¢æŠ¥é”™ï¼‰ï¼›å¦åˆ™æŒ‰ç©ºæ ¼åˆ‡åˆ†å†åˆå¹¶\n",
    "        hist = str(row['history']).split() if pd.notna(row['history']) else ['N0'] # hist = ['N123', 'N456', 'N789'] æˆ–è€…['N0']\n",
    "        hist_str = \" \".join(hist) # hist_str = \"N123 N456 N789\" æˆ–è€… \"N0\"  ->å°†histåˆ—è¡¨ä¸­æ‰€æœ‰å…ƒç´ è¿ç»“èµ·æ¥ç”Ÿæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸­é—´ç”¨ç©ºæ ¼éš”å¼€\n",
    "        # å¤„ç†impressions String->List (impressions = \"N78206-0 N26368-0 N7578-1\")\n",
    "        impressions = str(row['impressions']).split() #impressions = ['N78206-0', 'N26368-0', 'N7578-1']\n",
    "        \n",
    "        pos_list = [] #æ­£æ ·æœ¬åˆ—è¡¨\n",
    "        neg_list = [] #è´Ÿæ ·æœ¬åˆ—è¡¨\n",
    "\n",
    "        # 1. å…ˆæŠŠæœ¬æ¬¡æ›å…‰çš„æ‰€æœ‰æ–‡ç« æŒ‰æ­£è´Ÿåˆ†ç±»\n",
    "        for imp in impressions: # éå†è¯¥ç”¨æˆ·è¿™ä¸€æ¬¡åˆ·åˆ°çš„æ‰€æœ‰æ–°é—» #impressions = ['N78206-0', 'N26368-0', 'N7578-1'] æˆ–è€… ['N78206', 'N7578-1']\n",
    "            if '-' not in imp: continue    #è·³è¿‡['N78206']æ— æ•ˆè®°å½•\n",
    "            news_id, label = imp.split('-')# \"N78206-0\" -> ['N78206','0'], news_id = \"N78206\" ,label = \"0\"\n",
    "            label = int(label)  # \"0\"-> 0\n",
    "            if label == 1:\n",
    "                pos_list.append(news_id) # ç”¨æˆ·ç‚¹çš„\n",
    "            else:\n",
    "                neg_list.append(news_id) # ç”¨æˆ·æ²¡ç‚¹çš„\n",
    "        \n",
    "        # 2. æ„å»ºè®­ç»ƒæ ·æœ¬ ï¼ˆè®­ç»ƒé›†æ ¸å¿ƒç­–ç•¥ï¼š1:4 è´Ÿé‡‡æ ·ï¼‰\n",
    "        if mode == 'train':\n",
    "            for pos_news in pos_list:\n",
    "                # åŠ å…¥æ­£æ ·æœ¬\n",
    "                samples.append([user_id, hist_str, pos_news, 1])\n",
    "                \n",
    "                # è´Ÿé‡‡æ ·ï¼šä¸ºæ¯ä¸ªæ­£æ ·æœ¬é… N=neg_ratio ä¸ªè´Ÿæ ·æœ¬\n",
    "                # å¦‚æœè´Ÿæ ·æœ¬ä¸å¤Ÿï¼Œå°±é‡å¤é‡‡æ ·ï¼›å¦‚æœå¤Ÿï¼Œå°±éšæœºé‡‡ä¸é‡å¤çš„\n",
    "                if len(neg_list) >= neg_ratio:\n",
    "                    negs = random.sample(neg_list, neg_ratio)\n",
    "                else:\n",
    "                    # è´Ÿæ ·æœ¬ä¸å¤Ÿï¼Œå¾ªç¯å¡«å…… ï¼ˆæ•´æ•°å€å¤åˆ¶ {neg_ratio // len(neg_list)} + ä½™æ•°è¡¥é½{:neg_ratio % len(neg_list)}ï¼‰\n",
    "                    negs = neg_list * (neg_ratio // len(neg_list)) + neg_list[:neg_ratio % len(neg_list)]\n",
    "                \n",
    "                for neg_news in negs:\n",
    "                    samples.append([user_id, hist_str, neg_news, 0])\n",
    "        else:\n",
    "            # éªŒè¯é›†/æµ‹è¯•é›†ï¼šä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œä¸åšé‡‡æ ·\n",
    "            for imp in impressions:\n",
    "                if '-' not in imp: continue\n",
    "                news_id, label = imp.split('-')\n",
    "                samples.append([user_id, hist_str, news_id, int(label)])\n",
    "            \n",
    "    return pd.DataFrame(samples, columns=['user_id', 'history_str', 'news_id', 'label'])\n",
    "\n",
    "# --- é‡æ–°ç”Ÿæˆæ•°æ® ---\n",
    "print(\"æ­£åœ¨é‡æ–°å¤„ç†æ•°æ® (1:4 é‡‡æ ·)...\")\n",
    "# ä½¿ç”¨å…¨é‡æ•°æ®\n",
    "train_df = process_mind_data_ratio(train_behaviors, mode='train', neg_ratio=4)\n",
    "val_df = process_mind_data_ratio(valid_behaviors, mode='valid') # éªŒè¯é›†ä¿æŒåŸæ ·\n",
    "\n",
    "print(f\"\\næœ€ç»ˆè®­ç»ƒé›†å¤§å°: {len(train_df)}\")\n",
    "print(\"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ (åº”æ¥è¿‘ 1:4):\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec4153-bc6a-4ae1-ae90-dfc16df1b4bf",
   "metadata": {},
   "source": [
    "### ç¬¬ 2 æ­¥ï¼šID ç¼–ç  (Label Encoding)\n",
    "\n",
    "æˆ‘ä»¬éœ€è¦æŠŠ User ID å’Œ News ID è½¬æ¢æˆæ•°å­—ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a27f965c-ff4e-46ef-a8bc-4ec437c2ff73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†åˆ†ç±»ç‰¹å¾...\n",
      "ç‰¹å¾å·¥ç¨‹å‡çº§å®Œæˆï¼šå·²åŠ å…¥ç±»åˆ«ç‰¹å¾ï¼\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# --- 1. ç¼–ç  User ID ---\n",
    "#æ ‡ç­¾ç¼–ç LabelEncoderï¼Œç»™æ¯ä¸ªç”¨æˆ·åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°ï¼ˆ0, 1, 2...ï¼‰\n",
    "lbe_user = LabelEncoder() #sklearn.preprocessing.LabelEncoder çš„ä¸€ä¸ªå®ä¾‹\n",
    "all_users = pd.concat([train_df['user_id'], val_df['user_id']]).unique() #æŠŠè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ‰€æœ‰ç”¨æˆ· ID æ‹¼åœ¨ä¸€èµ·å»é‡ï¼Œå»ºç«‹ä¸€ä¸ªâ€œç”¨æˆ·èŠ±åå†Œâ€ all_users\n",
    "lbe_user.fit(all_users) #å¯¹æ‰€æœ‰ç”¨æˆ·èŠ±åå†Œall_users åº”ç”¨ LabelEncoderï¼Œç»™æ¯ä¸ªç”¨æˆ·åˆ†é…äº†ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°ç¼–å·ï¼ˆä» 0 å¼€å§‹ï¼‰\n",
    "\n",
    "train_df['user_id_idx'] = lbe_user.transform(train_df['user_id'])\n",
    "val_df['user_id_idx'] = lbe_user.transform(val_df['user_id'])\n",
    "\n",
    "# --- 2. ç¼–ç  News ID ---\n",
    "#æ¨¡å‹éœ€è¦ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯èƒ½å‡ºç°çš„æ–°é—»çš„å­—å…¸ã€‚\n",
    "lbe_news = LabelEncoder #è¿™æ˜¯ä¸€ä¸ª LabelEncoder å®ä¾‹\n",
    "\n",
    "all_news_ids = set(train_df['news_id']) | set(val_df['news_id']) #æµ‹è¯•é›†å’ŒéªŒè¯é›†æ‰¾æ–°é—»id\n",
    "for h_str in train_df['history_str']: #åœ¨ç”¨æˆ·çš„å†å²è®°å½•é‡Œæ‰¾æ–°é—»id\n",
    "    all_news_ids.update(h_str.split())\n",
    "all_news_ids.add('N0') #åœ¨æ–°é—»idä¸­å¢åŠ ä¸€ä¸ªN0\n",
    "\n",
    "lbe_news.fit(list(all_news_ids)) #LabelEncoderå®ä¾‹ lbe_news è®°ä½äº†æ‰€æœ‰çš„å»é‡åçš„æ–°é—» IDï¼Œå¹¶ç¼–ç  (0,1,2,...)\n",
    "vocab_size_news = len(lbe_news.classes_) + 1 #æ‰€æœ‰æ–°é—»çš„æ€»æ•°é‡ + ä¸€ä¸ªN0å ä½ç¬¦\n",
    "\n",
    "# News ID çš„æ˜ å°„å­—å…¸ï¼š news_map = {\"N001\": 1,\"N002\":2,... }\n",
    "news_map = dict(zip(lbe_news.classes_, range(1, len(lbe_news.classes_) + 1))) \n",
    "#ç”Ÿæˆçš„ news_map æ˜¯ä» 1 å¼€å§‹ç¼–å·çš„ï¼ˆrange(1, ...)ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†æŠŠ 0 å·ç´¢å¼•ç•™ç»™ Paddingï¼ˆå¡«å……å€¼ï¼‰\n",
    "\n",
    "# --- 3. å»ºç«‹â€œæ–°é—» -> ç±»åˆ«â€çš„æŸ¥æ‰¾è¡¨ ---\n",
    "#å¤„ç† Category å’Œ SubCategory \n",
    "\n",
    "#ç°å®æ•°æ®å¾€å¾€æ˜¯ä¸å®Œæ•´çš„ã€‚æœ‰äº›æ–°é—»å¯èƒ½æ²¡æœ‰åˆ†ç±»ï¼Œæˆ‘ä»¬ä¸èƒ½ç•™ç©ºï¼Œå¿…é¡»å¡«ä¸€ä¸ªé»˜è®¤å€¼ 'UNK' (Unknown)ï¼Œå¦åˆ™åç»­ç¼–ç ä¼šæŠ¥é”™ã€‚\n",
    "all_news['category'] = all_news['category'].fillna('UNK')\n",
    "all_news['subcategory'] = all_news['subcategory'].fillna('UNK')\n",
    "\n",
    "# å»ºç«‹ NewsID -> Category çš„æ˜ å°„è¡¨\n",
    "lbe_cat = LabelEncoder()\n",
    "lbe_subcat = LabelEncoder()\n",
    "all_news['cat_idx'] = lbe_cat.fit_transform(all_news['category']) + 1 # 0åšpadding\n",
    "all_news['subcat_idx'] = lbe_subcat.fit_transform(all_news['subcategory']) + 1\n",
    "\n",
    "vocab_size_cat = len(lbe_cat.classes_) + 1 #categoryçš„ç±»åˆ«æ•°é‡+ä¸€ä¸ªN0å ä½ç¬¦\n",
    "vocab_size_subcat = len(lbe_subcat.classes_) + 1 #subcategoryçš„ç±»åˆ«æ•°é‡+ä¸€ä¸ªN0å ä½ç¬¦\n",
    "\n",
    "# åˆ›å»ºå¿«é€ŸæŸ¥æ‰¾å­—å…¸ï¼ˆæ˜ å°„å­—å…¸ï¼‰: news2catå’Œnews2subcat\n",
    "#NewsID_String -> Cat_Int\n",
    "news2cat = dict(zip(all_news['news_id'], all_news['cat_idx']))\n",
    "news2subcat = dict(zip(all_news['news_id'], all_news['subcat_idx']))\n",
    "# è¿˜è¦åŠ ä¸Š N0 (padding)\n",
    "news2cat['N0'] = 0\n",
    "news2subcat['N0'] = 0\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šæŠŠ NewsID åˆ—è¡¨è½¬æ¢ä¸º Feature åˆ—è¡¨\n",
    "def map_ids_to_features(id_list, mapping_dict):\n",
    "    return [mapping_dict.get(x, 0) for x in id_list]\n",
    "    #è¿™æ˜¯ä¸ºäº†å¤„ç†**â€œæœªçŸ¥/å†·å¯åŠ¨â€**ç‰©å“ã€‚å¦‚æœæŸä¸ª ID åœ¨å­—å…¸é‡Œæ‰¾ä¸åˆ°ï¼ˆä¾‹å¦‚æµ‹è¯•é›†é‡Œçš„æ–°æ–°é—»ï¼‰ï¼Œå®ƒä¼šè¿”å›é»˜è®¤å€¼ 0ã€‚\n",
    "\n",
    "\n",
    "# ç¼–ç è®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®çš„new_id_idx\n",
    "train_df['news_id_idx'] = train_df['news_id'].apply(lambda x: news_map.get(x, 0))\n",
    "val_df['news_id_idx'] = val_df['news_id'].apply(lambda x: news_map.get(x, 0))\n",
    "\n",
    "# ç¼–ç è®­ç»ƒæ•°æ®ã€æµ‹è¯•æ•°æ®çš„ç±»åˆ«/å­ç±»åˆ«ç‰¹å¾\n",
    "train_df['cat_idx'] = train_df['news_id'].apply(lambda x: news2cat.get(x, 0))\n",
    "val_df['cat_idx'] = val_df['news_id'].apply(lambda x: news2cat.get(x, 0))\n",
    "train_df['subcat_idx'] = train_df['news_id'].apply(lambda x: news2subcat.get(x, 0))\n",
    "val_df['subcat_idx'] = val_df['news_id'].apply(lambda x: news2subcat.get(x, 0))\n",
    "\n",
    "## â€”â€”â€”â€”æ„å»ºç”¨æˆ·å†å²è¡Œä¸ºçš„å¤šæ¨¡æ€åºåˆ—ç‰¹å¾â€”â€”â€”â€”â€”â€”ç”¨æˆ·çš„ä¸€ä¸²æ–°é—» ID å†å²ï¼Œè½¬åŒ–æˆäº†ä¸‰ä¸²å¹³è¡Œçš„åºåˆ—ï¼šID åºåˆ—ã€ç±»åˆ«åºåˆ—å’Œå­ç±»åˆ«åºåˆ—ã€‚\n",
    "\n",
    "def get_seq_features(hist_str, mapping_dict): \n",
    "    \"\"\"\n",
    "    è¾“å…¥ï¼šç”¨æˆ·å†å²æ–°é—»è®°å½•ã€æ–°é—»ç±»åˆ«å­—å…¸ï¼ˆä¸€çº§ç±»åˆ«/äºŒçº§ç±»åˆ«ï¼‰\n",
    "        hist_str: ç”¨æˆ·çš„å†å²è®°å½•å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ \"N123 N456 N789\"\n",
    "        mapping_dict: ä¸€ä¸ªæ˜ å°„å­—å…¸ã€‚æ¯”å¦‚ news2catï¼ˆæ–°é—»ID -> ç±»åˆ«IDï¼‰\n",
    "    è¾“å‡ºï¼šç”¨æˆ·å†å²æ–°é—»è®°å½•å¯¹åº”çš„æ–°é—»ç±»åˆ«åˆ—è¡¨ï¼ˆä¸€çº§ç±»åˆ«/äºŒçº§ç±»åˆ«ï¼‰\n",
    "    ['N123', 'N456', 'N789'] -> [5, 12, 3]\n",
    "    \"\"\"\n",
    "    raw_ids = hist_str.split()  #æŠŠå­—ç¬¦ä¸²åˆ‡åˆ†æˆåˆ—è¡¨ ['N123', 'N456', 'N789']\n",
    "    #éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ–°é—» IDï¼Œå»å­—å…¸é‡ŒæŸ¥å®ƒçš„å±æ€§ï¼ˆç±»åˆ«æˆ–å­ç±»åˆ«ï¼‰ã€‚å¦‚æœæŸ¥ä¸åˆ°ï¼ˆæ¯”å¦‚æ˜¯ä¸ªæ–° IDï¼‰ï¼Œå°±è¿”å› 0\n",
    "    feats = [mapping_dict.get(x, 0) for x in raw_ids] \n",
    "    return feats\n",
    "\n",
    "# è®­ç»ƒé›†æµ‹è¯•é›†ä¸­ ç”¨æˆ·å†å²è¡Œä¸ºHistory News IDç¼–ç \n",
    "train_df['hist_idx_list'] = train_df['history_str'].apply(lambda x: [news_map.get(i, 0) for i in x.split()])\n",
    "val_df['hist_idx_list'] = val_df['history_str'].apply(lambda x: [news_map.get(i, 0) for i in x.split()])\n",
    "\n",
    "# è®­ç»ƒé›†æµ‹è¯•é›†ä¸­ ç”¨æˆ·å†å²è¡Œä¸º History Categoryç±»åˆ«ç¼–ç ï¼ˆä¸€çº§ç±»åˆ«ï¼‰\n",
    "train_df['hist_cat_list'] = train_df['history_str'].apply(lambda x: get_seq_features(x, news2cat))\n",
    "val_df['hist_cat_list'] = val_df['history_str'].apply(lambda x: get_seq_features(x, news2cat))\n",
    "\n",
    "# è®­ç»ƒé›†æµ‹è¯•é›†ä¸­ ç”¨æˆ·å†å²è¡Œä¸º History SubCategoryç±»åˆ«ç¼–ç ï¼ˆä¸€çº§ç±»åˆ«ï¼‰\n",
    "train_df['hist_subcat_list'] = train_df['history_str'].apply(lambda x: get_seq_features(x, news2subcat))\n",
    "val_df['hist_subcat_list'] = val_df['history_str'].apply(lambda x: get_seq_features(x, news2subcat))\n",
    "\n",
    "## ç”¨æˆ·å†å²è¡Œä¸ºçš„ Padding & Truncating å¤„ç†\n",
    "#å¯¹å˜é•¿çš„åºåˆ—æ•°æ®è¿›è¡Œâ€œæ ‡å‡†åŒ–â€å¤„ç†ï¼Œä½¿å…¶å˜æˆå›ºå®šé•¿åº¦ï¼ˆFixed Lengthï¼‰ï¼Œä»¥ä¾¿è¾“å…¥åˆ°æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚ DINã€DeepFMï¼‰ä¸­ã€‚\n",
    "#ç¥ç»ç½‘ç»œé€šå¸¸è¦æ±‚è¾“å…¥çŸ©é˜µçš„ç»´åº¦æ˜¯å›ºå®šçš„ï¼Œè€Œç”¨æˆ·çš„å†å²æµè§ˆè®°å½•æœ‰çš„é•¿ã€æœ‰çš„çŸ­ï¼Œæ‰€ä»¥å¿…é¡»è¿›è¡Œ Paddingï¼ˆå¡«å……ï¼‰ æˆ– Truncatingï¼ˆæˆªæ–­ï¼‰\n",
    "\n",
    "SEQ_LEN = 20#å®šä¹‰åºåˆ—çš„æœ€å¤§é•¿åº¦ä¸º 20\n",
    "def pad_col(df, col_name):\n",
    "    return list(pad_sequences(df[col_name].tolist(), maxlen=SEQ_LEN, padding='post', truncating='post', value=0))\n",
    "\n",
    "train_df['history_seq'] = pad_col(train_df, 'hist_idx_list')\n",
    "val_df['history_seq'] = pad_col(val_df, 'hist_idx_list')\n",
    "train_df['hist_cat_seq'] = pad_col(train_df, 'hist_cat_list')\n",
    "val_df['hist_cat_seq'] = pad_col(val_df, 'hist_cat_list')\n",
    "train_df['hist_subcat_seq'] = pad_col(train_df, 'hist_subcat_list')\n",
    "val_df['hist_subcat_seq'] = pad_col(val_df, 'hist_subcat_list')\n",
    "\n",
    "print(\"ç‰¹å¾å·¥ç¨‹å‡çº§å®Œæˆï¼šå·²åŠ å…¥ç±»åˆ«ç‰¹å¾ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d306b8-cbcf-45fb-9ac4-28060a94ba93",
   "metadata": {},
   "source": [
    "### ç¬¬ 3 æ­¥ï¼šğŸŒŸ BERTè¯­ä¹‰ç‰¹å¾æå– (ç®€å†æ ¸å¿ƒäº®ç‚¹)\n",
    "\n",
    "è¿™æ˜¯ä½ ç®€å†ä¸­ \"åŸºäº BERT æå–æ–‡å­—å‘é‡\" çš„å®ç°éƒ¨åˆ†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ sentence-transformers æ¥æå–æ–°é—»æ ‡é¢˜çš„ Embeddingï¼Œå¹¶æ„å»ºä¸€ä¸ª Embedding Matrixã€‚\n",
    "\n",
    "##### åˆ©ç”¨é¢„è®­ç»ƒçš„ BERT æ¨¡å‹ï¼ˆå¤§è¯­è¨€æ¨¡å‹LLMï¼‰æå–æ–°é—»æ ‡é¢˜çš„è¯­ä¹‰ç‰¹å¾ï¼Œå¹¶å°†å…¶å¯¹é½åˆ°æ¨èç³»ç»Ÿçš„ ID ç´¢å¼•ä¸Šï¼Œä½œä¸ºæ¨¡å‹ Embedding å±‚çš„åˆå§‹æƒé‡ï¼Œç”¨ BERT è¯»æ‡‚æ–°é—»æ ‡é¢˜ï¼Œç„¶åæŠŠè¯»æ‡‚çš„çŸ¥è¯†ï¼ˆå‘é‡ï¼‰å¡ç»™ DIN æ¨¡å‹ï¼Œè®©å®ƒèµ¢åœ¨èµ·è·‘çº¿ä¸Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a22dbe64-d50d-4c5a-8c93-de3dd1ce4934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News è¯è¡¨å¤§å°: 101288\n",
      "æ­£åœ¨åŠ è½½ BERT æ¨¡å‹...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11d3d50a280471f9397a0d68a92657f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/792 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT å‘é‡æ³¨å…¥å®Œæˆ (Index 1 to N)\n",
      "Embedding Matrix Shape: torch.Size([101288, 32])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch_rechub.utils.match import pad_sequences\n",
    "# é…ç½®\n",
    "\n",
    "USE_LLM = True # å¦‚æœç½‘ç»œä¸å¥½å…ˆè®¾ä¸º False\n",
    "\n",
    "EMBEDDING_DIM = 32\n",
    "# 1. ç¡®ä¿ vocab_size æ­£ç¡® (åŒ…å« 0 å· padding)\n",
    "# æ³¨æ„ï¼šlbe_news.classes_ æ˜¯æ‰€æœ‰æ–°é—»çš„åŸå§‹ ID\n",
    "vocab_size_news = len(lbe_news.classes_) + 1\n",
    "print(f\"News è¯è¡¨å¤§å°: {vocab_size_news}\")\n",
    "\n",
    "# 2. åˆå§‹åŒ– Embedding çŸ©é˜µ (å…¨ 0)\n",
    "# è¡Œæ•° = vocab_size, åˆ—æ•° = EMBEDDING_DIM\n",
    "pretrained_news_emb = np.zeros((vocab_size_news, EMBEDDING_DIM))\n",
    "\n",
    "if USE_LLM:\n",
    "    import os\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' # ä½¿ç”¨é•œåƒ\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"æ­£åœ¨åŠ è½½ BERT æ¨¡å‹...\")\n",
    "    model_bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    \n",
    "    # è·å–æ‰€æœ‰æ–°é—»æ ‡é¢˜ï¼Œä¿æŒé¡ºåº\n",
    "    titles = all_news.set_index('news_id').reindex(lbe_news.classes_)['title'].fillna(\"\").tolist()\n",
    "\n",
    "    # ç”Ÿæˆ Embedding\n",
    "    embeddings = model_bert.encode(titles, batch_size=128, show_progress_bar=True)\n",
    "\n",
    "    # é™ç»´\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=EMBEDDING_DIM)\n",
    "    embeddings_32 = pca.fit_transform(embeddings)\n",
    "\n",
    "    \n",
    "    # ã€æ ¸å¿ƒä¿®æ­£ç‚¹ã€‘\n",
    "    # lbe_news.classes_ ä¸­çš„ç¬¬ 0 ä¸ªæ–°é—»ï¼Œå¯¹åº”çš„ç¼–ç æ˜¯ 1 (å› ä¸ºæˆ‘ä»¬åœ¨ç‰¹å¾å·¥ç¨‹é‡Œ +1 äº†)\n",
    "    # æ‰€ä»¥ embeddings_32 çš„ç¬¬ 0 è¡Œï¼Œåº”è¯¥å¡«å…¥ pretrained_news_emb çš„ç¬¬ 1 è¡Œ\n",
    "    pretrained_news_emb[1:] = embeddings_32\n",
    "    print(\"BERT å‘é‡æ³¨å…¥å®Œæˆ (Index 1 to N)\")\n",
    "\n",
    "else:\n",
    "    print(\"ä½¿ç”¨éšæœºåˆå§‹åŒ–å‘é‡æ¨¡æ‹Ÿ (æ¼”ç¤ºæ¨¡å¼)...\")\n",
    "    # ã€æ ¸å¿ƒä¿®æ­£ç‚¹ã€‘ä»ç´¢å¼• 1 å¼€å§‹å¡«å……ï¼Œç´¢å¼• 0 ä¿æŒä¸º 0 (Padding)\n",
    "    pretrained_news_emb[1:] = np.random.uniform(-0.1, 0.1, size=(vocab_size_news - 1, EMBEDDING_DIM))\n",
    "\n",
    "# è½¬ä¸º Tensor\n",
    "pretrained_news_emb_tensor = torch.FloatTensor(pretrained_news_emb)\n",
    "print(f\"Embedding Matrix Shape: {pretrained_news_emb_tensor.shape}\")\n",
    "\n",
    "from torch_rechub.models.ranking import DIN\n",
    "from torch_rechub.basic.features import SparseFeature, SequenceFeature\n",
    "from torch_rechub.trainers import CTRTrainer\n",
    "from torch_rechub.utils.data import DataGenerator\n",
    "import torch\n",
    "\n",
    "\n",
    "# 1. å®šä¹‰ç‰¹å¾åˆ—\n",
    "user_cols = [SparseFeature(\"user_id_idx\", vocab_size=len(lbe_user.classes_), embed_dim=32)]\n",
    "# Item ç‰¹å¾ (News ID + Category + SubCategory)\n",
    "item_cols = [\n",
    "    SparseFeature(\"news_id_idx\", vocab_size=vocab_size_news, embed_dim=32),\n",
    "    # ã€ä¿®æ”¹ã€‘ï¼Œä¸ news_id ä¿æŒä¸€è‡´\n",
    "    SparseFeature(\"cat_idx\", vocab_size=vocab_size_cat, embed_dim=32),       \n",
    "    SparseFeature(\"subcat_idx\", vocab_size=vocab_size_subcat, embed_dim=32)\n",
    "]\n",
    "\n",
    "\n",
    "# åºåˆ—ç‰¹å¾ (News ID + Category + SubCategory)\n",
    "history_cols = [\n",
    "    SequenceFeature(\"history_seq\", vocab_size=vocab_size_news, embed_dim=32, pooling=\"concat\", shared_with=\"news_id_idx\"),\n",
    "    # ã€ä¿®æ”¹ã€‘\n",
    "    SequenceFeature(\"hist_cat_seq\", vocab_size=vocab_size_cat, embed_dim=32, pooling=\"concat\", shared_with=\"cat_idx\"),\n",
    "    SequenceFeature(\"hist_subcat_seq\", vocab_size=vocab_size_subcat, embed_dim=32, pooling=\"concat\", shared_with=\"subcat_idx\")\n",
    "]\n",
    "\n",
    "features = user_cols + item_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b847b26e-fef2-4f8b-91bc-266783a71df2",
   "metadata": {},
   "source": [
    "### ç¬¬ 4 æ­¥ï¼šæ„å»º DIN æ¨¡å‹ (Torch-Rechub) AUC=0.712\n",
    "\n",
    "è¿™é‡Œæˆ‘ä»¬è¦åšä¸€ä¸ªå…³é”®æ“ä½œï¼šå°†æˆ‘ä»¬ç”Ÿæˆçš„ pretrained_news_emb_tensor æ³¨å…¥åˆ° DIN æ¨¡å‹ä¸­ï¼Œä½œä¸ºæ–°é—» ID çš„ Embedding åˆå§‹åŒ–æƒé‡ã€‚è¿™å°±åœ¨ä»£ç å±‚é¢å®ç°äº†â€œLLM å¢å¼ºæ¨èâ€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85e3c427-f56f-4d16-aa0f-965fdff0c801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» æ­£åœ¨æ„å»º DataLoader (Mac æœ¬åœ°æ¨¡å¼)...\n",
      "æ­£åœ¨æ„å»ºæ¨¡å‹...\n",
      "âœ… BERT Embedding æ³¨å…¥æˆåŠŸ\n",
      "ğŸ æ£€æµ‹åˆ° MacOS MPS (Metal) åŠ é€Ÿå¼€å¯ï¼\n",
      "\n",
      "ğŸ”¥ [é˜¶æ®µä¸€] å¼€å§‹ Warm Up (å†»ç»“ BERT)...\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [40:48<00:00,  1.69it/s, loss=0.443]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:23<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 validation: auc: 0.6789376735356462\n",
      "\n",
      "ğŸš€ [é˜¶æ®µäºŒ] å¼€å§‹ Fine-tuning (è§£å†» BERTï¼Œå¯»æ‰¾æœ€ä½³å‚æ•°)...\n",
      "\n",
      ">>> Training Epoch 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [42:20<00:00,  1.63it/s, loss=0.435]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:38<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 1 Validation AUC: 0.68580\n",
      "    ğŸ† åˆ›æ–°é«˜ï¼æ¨¡å‹å·²ä¿å­˜ (AUC: 0.68580)\n",
      "\n",
      ">>> Training Epoch 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [41:44<00:00,  1.65it/s, loss=0.427]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:26<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 2 Validation AUC: 0.69336\n",
      "    ğŸ† åˆ›æ–°é«˜ï¼æ¨¡å‹å·²ä¿å­˜ (AUC: 0.69336)\n",
      "\n",
      ">>> Training Epoch 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [42:06<00:00,  1.64it/s, loss=0.433]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:28<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 3 Validation AUC: 0.70623\n",
      "    ğŸ† åˆ›æ–°é«˜ï¼æ¨¡å‹å·²ä¿å­˜ (AUC: 0.70623)\n",
      "\n",
      ">>> Training Epoch 4/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [41:24<00:00,  1.66it/s, loss=0.426]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:28<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 4 Validation AUC: 0.71329\n",
      "    ğŸ† åˆ›æ–°é«˜ï¼æ¨¡å‹å·²ä¿å­˜ (AUC: 0.71329)\n",
      "\n",
      ">>> Training Epoch 5/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [42:08<00:00,  1.63it/s, loss=0.421]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [16:26<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 5 Validation AUC: 0.71317\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "\n",
      ">>> Training Epoch 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [41:34<00:00,  1.66it/s, loss=0.425]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [11:58<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 6 Validation AUC: 0.70955\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "\n",
      ">>> Training Epoch 7/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [45:04<00:00,  1.53it/s, loss=0.413]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [12:04<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 7 Validation AUC: 0.70795\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "\n",
      ">>> Training Epoch 8/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [45:28<00:00,  1.51it/s, loss=0.408]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [11:49<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 8 Validation AUC: 0.70231\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "\n",
      ">>> Training Epoch 9/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [48:32<00:00,  1.42it/s, loss=0.403]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [12:01<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 9 Validation AUC: 0.70299\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "\n",
      ">>> Training Epoch 10/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4131/4131 [48:46<00:00,  1.41it/s, loss=0.389]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [12:13<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch 10 Validation AUC: 0.70060\n",
      "    (æœªè¶…è¿‡æœ€ä½³ AUC: 0.71329 @ Epoch 4)\n",
      "========================================\n",
      "ğŸ‰ è®­ç»ƒç»“æŸï¼\n",
      "æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ Epoch 4ï¼ŒAUC ä¸º 0.71329\n",
      "ä½ å¯ä»¥ä½¿ç”¨ model_din.load_state_dict(torch.load('din_best_model.pth')) åŠ è½½æœ€ä½³çŠ¶æ€ã€‚\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 2. å‡†å¤‡æ•°æ® (Mac ä¼˜åŒ–ç‰ˆ)\n",
    "# ==========================================\n",
    "print(\"ğŸ’» æ­£åœ¨æ„å»º DataLoader (Mac æœ¬åœ°æ¨¡å¼)...\")\n",
    "\n",
    "# å®šä¹‰åˆ—\n",
    "input_cols = ['user_id_idx', 'news_id_idx', 'cat_idx', 'subcat_idx', 'history_seq', 'hist_cat_seq', 'hist_subcat_seq']\n",
    "\n",
    "train_dg = DataGenerator(x=train_df[input_cols], y=train_df['label'])\n",
    "val_dg = DataGenerator(x=val_df[input_cols], y=val_df['label'])\n",
    "\n",
    "# ã€Mac è®¾ç½®å»ºè®®ã€‘\n",
    "# 1. num_workers=0: Mac åœ¨ Jupyter é‡Œå¤šè¿›ç¨‹å®¹æ˜“æŠ¥ BrokenPipeï¼Œè®¾ä¸º 0 æœ€ç¨³ã€‚\n",
    "# 2. batch_size=4096: Mac å†…å­˜é€šå¸¸ç»Ÿä¸€ï¼Œé€‚å½“è°ƒå¤§å¯ä»¥åŠ é€Ÿ (æ¯” 1024 å¿«)ã€‚\n",
    "train_loader, val_loader, _ = train_dg.generate_dataloader(\n",
    "    x_val=val_df[input_cols], \n",
    "    y_val=val_df['label'], \n",
    "    batch_size=4096, \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 3. å®šä¹‰æ¨¡å‹ & æ³¨å…¥æƒé‡\n",
    "# ==========================================\n",
    "print(\"æ­£åœ¨æ„å»ºæ¨¡å‹...\")\n",
    "model_din = DIN(\n",
    "    features=features,\n",
    "    history_features=history_cols,\n",
    "    target_features=item_cols,\n",
    "    mlp_params={\"dims\": [256, 128], \"dropout\": 0.3},\n",
    "    attention_mlp_params={\"dims\": [64, 32]}\n",
    ")\n",
    "\n",
    "# --- æ³¨å…¥ BERT æƒé‡ ---\n",
    "target_layer_found = False\n",
    "embedding_weight_param = None \n",
    "\n",
    "for name, param in model_din.named_parameters():\n",
    "    if \"news_id_idx\" in name and \"weight\" in name:\n",
    "        param.data.copy_(pretrained_news_emb_tensor)\n",
    "        embedding_weight_param = param \n",
    "        target_layer_found = True\n",
    "        print(\"âœ… BERT Embedding æ³¨å…¥æˆåŠŸ\")\n",
    "        break\n",
    "if not target_layer_found:\n",
    "    print(\"âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ° Embedding å±‚ï¼\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥ (è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹)\n",
    "# ==========================================\n",
    "# Mac Mç³»åˆ—èŠ¯ç‰‡æ”¯æŒ mps åŠ é€Ÿï¼ŒIntelèŠ¯ç‰‡ç”¨ cpu\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = 'mps'\n",
    "    print(\"ğŸ æ£€æµ‹åˆ° MacOS MPS (Metal) åŠ é€Ÿå¼€å¯ï¼\")\n",
    "\n",
    "# ------------------------------------\n",
    "# é˜¶æ®µä¸€ï¼šWarm Up (å†»ç»“ BERT)\n",
    "# ------------------------------------\n",
    "print(\"\\nğŸ”¥ [é˜¶æ®µä¸€] å¼€å§‹ Warm Up (å†»ç»“ BERT)...\")\n",
    "\n",
    "if embedding_weight_param is not None:\n",
    "    embedding_weight_param.requires_grad = False\n",
    "\n",
    "# Warmup åªè·‘ 1 è½®ï¼Œä¸éœ€è¦å­˜æ¨¡å‹ï¼Œç›´æ¥ fit\n",
    "trainer_warmup = CTRTrainer(\n",
    "    model=model_din,\n",
    "    optimizer_params={\"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "    n_epoch=1, \n",
    "    device=device\n",
    ")\n",
    "trainer_warmup.fit(train_dataloader=train_loader, val_dataloader=val_loader)\n",
    "\n",
    "# ------------------------------------\n",
    "# é˜¶æ®µäºŒï¼šFine-tuning (è§£å†» BERT + æ“‚å°èµ›ä¿å­˜æ³•)\n",
    "# ------------------------------------\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"\\nğŸš€ [é˜¶æ®µäºŒ] å¼€å§‹ Fine-tuning (è§£å†» BERTï¼Œå¯»æ‰¾æœ€ä½³å‚æ•°)...\")\n",
    "\n",
    "# 1. è§£å†» Embedding\n",
    "if embedding_weight_param is not None:\n",
    "    embedding_weight_param.requires_grad = True\n",
    "\n",
    "# 2. å®šä¹‰ä¼˜åŒ–å™¨å‚æ•°\n",
    "# æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶å¾ªç¯ï¼Œæ‰€ä»¥ n_epoch åœ¨ Trainer é‡Œè®¾ä¸º 1 å³å¯ï¼Œç”±å¤–å±‚å¾ªç¯æ§åˆ¶æ€»è½®æ•°\n",
    "trainer_finetune = CTRTrainer(\n",
    "    model=model_din,\n",
    "    optimizer_params={\"lr\": 5e-5, \"weight_decay\": 1e-4}, \n",
    "    n_epoch=1, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 3. åˆå§‹åŒ–æœ€ä½³è®°å½•\n",
    "best_auc = 0.0\n",
    "best_epoch = 0\n",
    "total_epochs = 10  # è®¡åˆ’è·‘ 10 è½®\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    print(f\"\\n>>> Training Epoch {epoch + 1}/{total_epochs}...\")\n",
    "    \n",
    "    # 1. è®­ç»ƒä¸€è½®\n",
    "    # torch_rechub çš„ train_one_epoch é€šå¸¸ä¸éœ€è¦ä¼  modelï¼Œå®ƒç”¨ self.model\n",
    "    trainer_finetune.train_one_epoch(train_loader)\n",
    "    \n",
    "    # 2. éªŒè¯ä¸€è½® (è·å–å½“å‰ AUC)\n",
    "    # ã€æ ¸å¿ƒä¿®å¤ã€‘å¿…é¡»ä¼ å…¥ model_din ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ï¼\n",
    "    current_auc = trainer_finetune.evaluate(model_din, val_loader)\n",
    "    \n",
    "    print(f\"    Epoch {epoch + 1} Validation AUC: {current_auc:.5f}\")\n",
    "    \n",
    "    # 3. æ“‚å°èµ›ï¼šå¦‚æœå½“å‰ AUC è¶…è¿‡å†å²æœ€é«˜\n",
    "    if current_auc > best_auc:\n",
    "        best_auc = current_auc\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        # ä¿å­˜è¿™ä¸€è½®çš„æ¨¡å‹ä¸º \"æœ€ä½³æ¨¡å‹\"\n",
    "        torch.save(model_din.state_dict(), 'din_best_model.pth')\n",
    "        print(f\"    ğŸ† åˆ›æ–°é«˜ï¼æ¨¡å‹å·²ä¿å­˜ (AUC: {best_auc:.5f})\")\n",
    "    else:\n",
    "        print(f\"    (æœªè¶…è¿‡æœ€ä½³ AUC: {best_auc:.5f} @ Epoch {best_epoch})\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(f\"ğŸ‰ è®­ç»ƒç»“æŸï¼\")\n",
    "print(f\"æœ€ä½³æ¨¡å‹å‡ºç°åœ¨ Epoch {best_epoch}ï¼ŒAUC ä¸º {best_auc:.5f}\")\n",
    "print(\"ä½ å¯ä»¥ä½¿ç”¨ model_din.load_state_dict(torch.load('din_best_model.pth')) åŠ è½½æœ€ä½³çŠ¶æ€ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "89b0682f-5db0-45cc-9bc6-becf10025912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š æ­£åœ¨è¿›è¡Œå…¨é‡å¤šç»´åº¦è¯„ä¼°...\n",
      "æ­£åœ¨é¢„æµ‹éªŒè¯é›†åˆ†æ•°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3439/3439 [11:17<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° impression_idï¼Œä½¿ç”¨ user_id è¿‘ä¼¼åˆ†ç»„\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Ranking Metrics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 255990/255990 [01:51<00:00, 2299.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "âœ… æœ€ç»ˆè¯„ä¼°ç»“æœ (Validation Set):\n",
      "Global AUC (LogLoss): 0.7133\n",
      "Group AUC (GAUC)    : 0.6932\n",
      "MRR                 : 0.3818\n",
      "nDCG@5              : 0.3305\n",
      "nDCG@10             : 0.3917\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "model_din.load_state_dict(torch.load('din_best_model.pth'))\n",
    "# ==========================================\n",
    "# 5. å¤šç»´åº¦è¯„ä¼°å‡½æ•° (é€‚é… NumPy 2.0)\n",
    "# ==========================================\n",
    "\n",
    "def calculate_metrics(grouped_df):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å•ä¸ªæ›å…‰åˆ—è¡¨çš„æŒ‡æ ‡\n",
    "    \"\"\"\n",
    "    aucs, mrrs, ndcg5s, ndcg10s = [], [], [], []\n",
    "    \n",
    "    # éå†æ¯ä¸ªç”¨æˆ·çš„æ›å…‰åˆ—è¡¨\n",
    "    for _, group in tqdm(grouped_df, desc=\"Evaluating Ranking Metrics\"):\n",
    "        labels = group['label'].values\n",
    "        preds = group['pred'].values\n",
    "        \n",
    "        # 1. å¦‚æœå…¨æ˜¯è´Ÿæ ·æœ¬æˆ–å…¨æ˜¯æ­£æ ·æœ¬ï¼Œæ— æ³•è®¡ç®— AUCï¼Œè·³è¿‡\n",
    "        if len(np.unique(labels)) == 1:\n",
    "            continue\n",
    "            \n",
    "        # --- AUC ---\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(labels, preds))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        # --- æ’åºé¢„å¤„ç† ---\n",
    "        # æ ¹æ®é¢„æµ‹åˆ†æ•°ä»é«˜åˆ°ä½æ’åº\n",
    "        sorted_indices = np.argsort(preds)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # --- MRR (Mean Reciprocal Rank) ---\n",
    "        first_pos_idx = np.where(sorted_labels == 1)[0]\n",
    "        if len(first_pos_idx) > 0:\n",
    "            mrrs.append(1.0 / (first_pos_idx[0] + 1))\n",
    "        else:\n",
    "            mrrs.append(0.0)\n",
    "            \n",
    "        # --- nDCG (Normalized Discounted Cumulative Gain) ---\n",
    "        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— DCG\n",
    "        def dcg_at_k(r, k):\n",
    "            # ã€ä¿®å¤ç‚¹ã€‘np.asfarray -> np.asarray(..., dtype=float)\n",
    "            r = np.asarray(r, dtype=float)[:k]\n",
    "            if r.size:\n",
    "                return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "            return 0.\n",
    "\n",
    "        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— nDCG\n",
    "        def ndcg_at_k(r, k):\n",
    "            dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "            if not dcg_max:\n",
    "                return 0.\n",
    "            return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "        ndcg5s.append(ndcg_at_k(sorted_labels, 5))\n",
    "        ndcg10s.append(ndcg_at_k(sorted_labels, 10))\n",
    "        \n",
    "    return np.mean(aucs), np.mean(mrrs), np.mean(ndcg5s), np.mean(ndcg10s)\n",
    "\n",
    "# ==========================================\n",
    "# 6. æ‰§è¡Œè¯„ä¼° (ä¿®å¤ predict å‚æ•°)\n",
    "# ==========================================\n",
    "print(\"\\nğŸ“Š æ­£åœ¨è¿›è¡Œå…¨é‡å¤šç»´åº¦è¯„ä¼°...\")\n",
    "\n",
    "# 1. è·å–æ¨¡å‹é¢„æµ‹åˆ†\n",
    "model_din.eval() \n",
    "\n",
    "print(\"æ­£åœ¨é¢„æµ‹éªŒè¯é›†åˆ†æ•°...\")\n",
    "# ã€æ³¨æ„ã€‘å¿…é¡»ä¼ å…¥ model_din ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°\n",
    "y_pred = trainer_finetune.predict(model_din, val_loader) \n",
    "\n",
    "# 2. å°†é¢„æµ‹åˆ†æ‹¼å›åˆ°éªŒè¯é›† DataFrame\n",
    "eval_df = val_df.copy()\n",
    "eval_df['pred'] = np.array(y_pred)\n",
    "\n",
    "# 3. ç¡®å®šåˆ†ç»„åˆ—\n",
    "if 'impression_id' not in eval_df.columns:\n",
    "    print(\"âš ï¸ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° impression_idï¼Œä½¿ç”¨ user_id è¿‘ä¼¼åˆ†ç»„\")\n",
    "    eval_df['group_id'] = eval_df['user_id'] \n",
    "else:\n",
    "    eval_df['group_id'] = eval_df['impression_id']\n",
    "\n",
    "# 4. åˆ†ç»„å¹¶è®¡ç®—\n",
    "grouped = eval_df.groupby('group_id')\n",
    "gauc, mrr, ndcg5, ndcg10 = calculate_metrics(grouped)\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(f\"âœ… æœ€ç»ˆè¯„ä¼°ç»“æœ (Validation Set):\")\n",
    "print(f\"Global AUC (LogLoss): {roc_auc_score(val_df['label'], eval_df['pred']):.4f}\")\n",
    "print(f\"Group AUC (GAUC)    : {gauc:.4f}\")\n",
    "print(f\"MRR                 : {mrr:.4f}\")\n",
    "print(f\"nDCG@5              : {ndcg5:.4f}\")\n",
    "print(f\"nDCG@10             : {ndcg10:.4f}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0425f2-4fa3-4b0b-a616-ded9dc10cae7",
   "metadata": {},
   "source": [
    "### DCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d11706-9fea-485e-bb2c-621feeff2870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b34d22ed-70de-4a61-9ddd-ed4485eda43e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found layer: embedding.embed_dict.news_id_idx.weight\n",
      "âœ… LLM Embedding å·²æˆåŠŸæ³¨å…¥ DCNï¼\n",
      "å¼€å§‹è®­ç»ƒ DCN æ¨¡å‹ (1:4 Data + LLM + Categories)...\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1155/1155 [00:42<00:00, 26.88it/s, loss=0.46]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2677/2677 [00:51<00:00, 51.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 validation: auc: 0.6168520333364222\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1155/1155 [00:36<00:00, 31.88it/s, loss=0.459]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2677/2677 [00:49<00:00, 53.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 validation: auc: 0.630628156808087\n",
      "epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1155/1155 [00:41<00:00, 27.74it/s, loss=0.45]\n",
      "validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2677/2677 [00:58<00:00, 46.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 validation: auc: 0.6257678667263062\n"
     ]
    }
   ],
   "source": [
    "from torch_rechub.models.ranking import DCN\n",
    "from torch_rechub.trainers import CTRTrainer\n",
    "import torch\n",
    "\n",
    "# 1. å®šä¹‰ DCN æ¨¡å‹\n",
    "# DCN = Deep Network (DNN) + Cross Network (æ˜¾å¼é«˜é˜¶äº¤äº’)\n",
    "# æ ¸å¿ƒå‚æ•°æ˜¯ n_cross_layers (äº¤å‰å±‚æ•°)ï¼Œé€šå¸¸è®¾ä¸º 2 æˆ– 3\n",
    "model_dcn = DCN(\n",
    "    features=features, \n",
    "    mlp_params={\"dims\": [256, 128], \"dropout\": 0.4}, # Deep éƒ¨åˆ†\n",
    "    n_cross_layers=3,                                # Cross éƒ¨åˆ† (3å±‚äº¤äº’)\n",
    ")\n",
    "\n",
    "# 2. æ³¨å…¥ LLM Embedding\n",
    "# DCN çš„ embedding å­˜å‚¨ä½ç½®å’Œ DIN ä¸€æ ·ï¼Œä¹Ÿæ˜¯åœ¨ model.embedding ä¸­\n",
    "target_layer_found = False\n",
    "\n",
    "for name, param in model_dcn.named_parameters():\n",
    "    # DCN çš„ç»“æ„é‡ŒåŒæ ·åŒ…å« embedding å±‚\n",
    "    if \"news_id_idx\" in name and \"weight\" in name:\n",
    "        print(f\"Found layer: {name}\")\n",
    "        param.data.copy_(pretrained_news_emb_tensor)\n",
    "        \n",
    "        # ã€ç­–ç•¥ã€‘è§£å†»ï¼Œå…è®¸å¾®è°ƒï¼Œè®© LLM å‘é‡é€‚åº” DCN çš„äº¤å‰ç»“æ„\n",
    "        param.requires_grad = True \n",
    "        target_layer_found = True\n",
    "        break\n",
    "\n",
    "if target_layer_found:\n",
    "    print(\"âœ… LLM Embedding å·²æˆåŠŸæ³¨å…¥ DCNï¼\")\n",
    "else:\n",
    "    print(\"âŒ æ³¨å…¥å¤±è´¥\")\n",
    "\n",
    "# 3. è®­ç»ƒ\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = 'mps'\n",
    "\n",
    "trainer = CTRTrainer(\n",
    "    model=model_dcn,\n",
    "    # æ²¿ç”¨ä½ è°ƒä¼˜è¿‡çš„å‚æ•°\n",
    "    optimizer_params={\"lr\": 5e-4, \"weight_decay\": 5e-4}, \n",
    "    n_epoch=3, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"å¼€å§‹è®­ç»ƒ DCN æ¨¡å‹ (1:4 Data + LLM + Categories)...\")\n",
    "trainer.fit(train_dataloader=train_loader, val_dataloader=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac53caf-9e48-4908-a8f3-67e6f8d2bf3c",
   "metadata": {},
   "source": [
    "## é—®é¢˜ä¸€ï¼šä¸ºä»€ä¹ˆ DCN è·‘ä¸è¿‡ DINï¼Ÿ\n",
    "\n",
    "### 1.æœºåˆ¶å·®å¼‚ï¼ˆAttention vs. Poolingï¼‰ï¼š\n",
    "\n",
    "DIN çš„æ ¸å¿ƒä¼˜åŠ¿ï¼šDIN ä½¿ç”¨äº† Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚å½“æ¨¡å‹åˆ¤æ–­â€œè¦ä¸è¦ç»™ç”¨æˆ·æ¨ä¸€æ¡ã€Šæ¹–äººé˜Ÿæ¯”èµ›ã€‹æ–°é—»â€æ—¶ï¼ŒDIN ä¼šå›å¤´çœ‹ç”¨æˆ·çš„å†å²ï¼Œè‡ªåŠ¨é«˜äº®å†å²ä¸­å’Œâ€œç¯®çƒâ€ç›¸å…³çš„è®°å½•ï¼Œå¿½ç•¥æ‰â€œç†è´¢â€ã€â€œå¤©æ°”â€ç­‰æ— å…³è®°å½•ã€‚è¿™å« Target-Awareï¼ˆç›®æ ‡ç›¸å…³æ€§ï¼‰ã€‚\n",
    "\n",
    "DCN çš„åŠ£åŠ¿ï¼šDCN è™½ç„¶å¼ºåœ¨ç‰¹å¾äº¤å‰ï¼Œä½†å®ƒå¤„ç†â€œå†å²è¡Œä¸ºåºåˆ—â€æ—¶ï¼Œé€šå¸¸åªèƒ½ç®€å•ç²—æš´åœ°æŠŠè¿‡å»çœ‹è¿‡çš„ 20 æ¡æ–°é—» å–å¹³å‡ï¼ˆMean Poolingï¼‰ æˆ–æ‹¼æ¥ã€‚è¿™å°±æŠŠç”¨æˆ·çš„å…´è¶£â€œç³Šæˆäº†ä¸€å›¢â€ã€‚\n",
    "\n",
    "ç»“æœï¼šåœ¨è¿™ä¸ªæ–°é—»æ•°æ®é›†ä¸Šï¼Œâ€œç²¾å‡†åŒ¹é…å…´è¶£â€ï¼ˆDINï¼‰æ¯” â€œå…¨å±€ç‰¹å¾äº¤å‰â€ï¼ˆDCNï¼‰æ›´é‡è¦ã€‚\n",
    "\n",
    "### 2.LLM å‘é‡çš„åˆ©ç”¨ç‡ï¼š\n",
    "\n",
    "BERT èµ‹äºˆäº† Embedding æå¼ºçš„è¯­ä¹‰ï¼ˆæ¯”å¦‚â€œè‹¹æœâ€å’Œâ€œiPhoneâ€å‘é‡å¾ˆè¿‘ï¼‰ã€‚\n",
    "\n",
    "DIN çš„ Attention æœºåˆ¶ç›´æ¥è®¡ç®— Target(è‹¹æœ) å’Œ History(iPhone) çš„å‘é‡ç›¸ä¼¼åº¦ï¼Œå®Œç¾åˆ©ç”¨äº† BERT çš„è¯­ä¹‰èƒ½åŠ›ã€‚\n",
    "\n",
    "DCN åªæ˜¯æŠŠè¿™äº›å‘é‡å½“æˆæ™®é€šæ•°å­—å»ä¹˜æ¥ä¹˜å»ï¼Œæµªè´¹äº† BERT å¸¦æ¥çš„è¯­ä¹‰å¯¹é½ä¼˜åŠ¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e437-8ef5-4fc0-97a8-fd05cf906ff2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## é—®é¢˜äºŒï¼šä½ æåˆ°çš„ COBRA æœºåˆ¶å…·ä½“æ˜¯æ€ä¹ˆåšçš„ï¼Ÿä½ ç”¨çš„æ•ˆæœä¸ºä»€ä¹ˆä¸å¥½\n",
    "\n",
    "COBRA è®ºæ–‡æå‡ºç”Ÿæˆå¼æ¨èéœ€è¦ç»“åˆ Sparse ID å’Œ Dense Vectorã€‚å—æ­¤å¯å‘ï¼Œæˆ‘è®¤ä¸ºåˆ¤åˆ«å¼æ¨èï¼ˆCTRï¼‰ä¹Ÿå¯ä»¥å€Ÿé‰´ã€‚æˆ‘æ²¡æœ‰ä½¿ç”¨è®ºæ–‡ä¸­å¤æ‚çš„ RQ-VAE ç”Ÿæˆ ID ï¼Œè€Œæ˜¯å·¥ç¨‹åŒ–åœ°ä½¿ç”¨äº†QA K-Means å¯¹ BERT å‘é‡è¿›è¡Œèšç±»ï¼ˆå°è¯•å¼•å…¥ RQ-KMeans (æ®‹å·®é‡åŒ–)ï¼Œè¯•å›¾å°†æ–°é—»çš„ BERT å‘é‡è½¬åŒ–ä¸ºç¦»æ•£çš„è¯­ä¹‰ IDï¼ˆCodebookï¼‰ã€‚â€ï¼‰ï¼Œç”Ÿæˆçš„ Cluster ID ä½œä¸ºä¸€ç§â€˜è¯­ä¹‰ IDâ€™è¾“å…¥åˆ° DIN çš„ç¨€ç–ç‰¹å¾å±‚ï¼ŒåŒæ—¶å°† BERT åŸå§‹å‘é‡è¾“å…¥åˆ°ç¨ å¯†å±‚ã€‚å°è¯•äº†åŒè·¯è¾“å…¥ã€‚\n",
    "\n",
    "### é˜¶æ®µä¸€ï¼šç¦»çº¿å‡†å¤‡ï¼ˆé€ â€œæ¡¶â€ï¼‰åœ¨æ¨¡å‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åˆ©ç”¨å†å²å­˜é‡çš„æ–‡ç« ï¼ˆæ¯”å¦‚ 10 ä¸‡ç¯‡ï¼‰åšå‡†å¤‡ï¼š\n",
    "\n",
    "#### ï¼ˆ1ï¼‰BERT å‘é‡åŒ–ï¼šRaw Text æ–°é—»æ ‡é¢˜(äººç±»çœ‹å¾—æ‡‚ï¼Œæœºå™¨çœ‹ä¸æ‡‚) -> Dense Vector è¯­ä¹‰å‘é‡ (æœºå™¨çœ‹æ‡‚äº†è¯­ä¹‰ï¼Œå˜æˆäº†é«˜ç»´åæ ‡)\n",
    "\n",
    "æŠŠè¿™ 10 ä¸‡ç¯‡æ–‡ç« çš„æ ‡é¢˜è¾“å…¥ BERTï¼Œå¾—åˆ° 10 ä¸‡ä¸ª768ç»´BERTå‘é‡ã€‚BERT ä½œä¸ºä¸€ä¸ªé¢„è®­ç»ƒå¤§æ¨¡å‹ï¼Œå®ƒç†è§£è¯­ä¹‰ï¼ï¼\n",
    "\n",
    "#### ï¼ˆ2ï¼‰PCAé™ç»´ï¼šDense Vectorè¯­ä¹‰å‘é‡ -> embeddings_reduced (æµ“ç¼©å‘é‡)\n",
    "\n",
    "BERT åŸç”Ÿè¾“å‡ºæ˜¯ 768 ç»´ã€‚ç›´æ¥å¯¹ 6.5 ä¸‡æ¡æ•°æ®åš 768 ç»´çš„ K-Means èšç±»è®¡ç®—é‡éå¸¸å¤§ï¼Œè€Œä¸”å®¹æ˜“å—â€œç»´åº¦ç¾éš¾â€å½±å“ï¼Œæ•ˆæœä¸ä¸€å®šå¥½ã€‚æ‰€ä»¥æˆ‘ä»¬å…ˆç”¨ PCA æŠŠå®ƒæµ“ç¼©æˆ 64 ç»´ (embeddings_reduced)ã€‚è¿™æ—¢ä¿ç•™äº†æ ¸å¿ƒè¯­ä¹‰ï¼Œåˆè®© K-Means è·‘å¾—æ›´å¿«ã€æ›´å‡†ã€‚\n",
    "\n",
    "#### ï¼ˆ3ï¼‰QR K-Means èšç±»ï¼šå¾—åˆ°Sparse IDs (è¯­ä¹‰ ID)ï¼Œæ ¹æ®åæ ‡ä½ç½®ï¼Œç»™å®ƒè´´ä¸Šæ ‡ç­¾ã€‚\n",
    "\n",
    "å¯¹è¿™ 10 ä¸‡ä¸ª64ç»´å‘é‡embeddings_reducedè¿›è¡Œèšç±»ï¼ˆå‡è®¾ $K=1000$ï¼‰ã€‚\n",
    "\n",
    "\n",
    "ä¿å­˜ä¸­å¿ƒç‚¹ï¼šæˆ‘ä»¬å¾—åˆ°äº† 32 ä¸ªèšç±»ä¸­å¿ƒï¼ˆCentroidsï¼‰ï¼Œå¹¶ç»™å®ƒä»¬ç¼–å· 0 åˆ° 31ã€‚è¿™äº›ç¼–å·å°±æ˜¯æˆ‘ä»¬çš„ Semantic Cluster IDã€‚\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒï¼šåœ¨è®­ç»ƒ DIN/DCN æ—¶ï¼Œæˆ‘ä»¬å°†è¿™ä¸ª Cluster_ID ä½œä¸ºä¸€ä¸ªç‰¹å¾è¾“å…¥ã€‚æ¨¡å‹ä¼šå­¦ä¹ åˆ°ç”¨æˆ·å¯¹è¿™äº›â€œç±»ç°‡â€çš„å–œå¥½ï¼ˆä¾‹å¦‚ï¼šCluster4ä»£è¡¨â€œäººå·¥æ™ºèƒ½â€ï¼Œç”¨æˆ·å–œæ¬¢çœ‹ï¼‰ã€‚\n",
    "\n",
    "### é˜¶æ®µäºŒï¼šå†·å¯åŠ¨æ¨æ–­ï¼ˆæ–°æ–‡ç« æ¥äº†ï¼ï¼‰ç°åœ¨ï¼Œä¸€ç¯‡å…³äºâ€œOpenAI å‘å¸ƒ Soraâ€çš„æ–°é—»æ¥äº†ï¼ˆè®­ç»ƒé›†ä¸­æ²¡è§è¿‡ï¼‰ã€‚\n",
    "\n",
    "è·å– Dense ç‰¹å¾ï¼šæŠŠæ ‡é¢˜æ‰”è¿› BERTï¼Œç›´æ¥ç®—å‡ºå®ƒçš„å‘é‡ $V_{new}$ã€‚è¿™å°±æ˜¯ Dense Featureã€‚å®ƒåŒ…å«äº†â€œè§†é¢‘ç”Ÿæˆã€AIâ€ç­‰ç»†ç²’åº¦è¯­ä¹‰ã€‚è·å– Sparse ID ç‰¹å¾ï¼šè®¡ç®— $V_{new}$ ä¸ä¹‹å‰ä¿å­˜çš„ 32 ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»ã€‚å‘ç°å®ƒç¦» Cluster 42ï¼ˆäººå·¥æ™ºèƒ½ç±»ï¼‰æœ€è¿‘ã€‚ç›´æ¥èµ‹äºˆå®ƒ ID = 42ã€‚è¿™å°±æ˜¯ Sparse ID Featureã€‚\n",
    "\n",
    "æ¨¡å‹æ¨ç†ï¼šè™½ç„¶æ¨¡å‹æ²¡è§è¿‡è¿™ç¯‡æ–‡ç« çš„ Item_IDï¼ˆæ­¤æ—¶é€šå¸¸ç”¨ UNK æˆ– Padding å¤„ç†ï¼Œä¿¡æ¯é‡ä¸º 0ï¼‰ï¼Œä½†æ¨¡å‹è§è¿‡ ID=4ã€‚æ¨¡å‹å¿ƒé‡Œæƒ³ï¼šâ€œå“¦ï¼Œè™½ç„¶ä¸çŸ¥é“è¿™æ˜¯å•¥æ–°é—»ï¼Œä½†å®ƒå±äºç¬¬ 4 ç±»ï¼Œè¿™ä¸ªç”¨æˆ·ä»¥å‰è€çœ‹ 4 ç±»çš„æ–°é—»ï¼Œæ‰€ä»¥æˆ‘åº”è¯¥æ¨èå®ƒï¼â€\n",
    "\n",
    "#### é‡åˆ°çš„æŒ‘æˆ˜ï¼š\n",
    "å®éªŒä¸­æˆ‘å‘ç°ï¼Œç›´æ¥ä½¿ç”¨ RQ-KMeans ç”Ÿæˆçš„ç¦»æ•£ ID è¿›è¡Œè®­ç»ƒï¼ŒAUC æŒ‡æ ‡å¹¶ä¸ç†æƒ³ï¼ˆå¾˜å¾Šåœ¨ 0.5~0.6ï¼‰ã€‚ç»è¿‡åˆ†æï¼Œæˆ‘è®¤ä¸ºåŸå› åœ¨äºï¼šç¡¬é‡åŒ–ï¼ˆHard Quantizationï¼‰å¯¼è‡´äº†ä¸¥é‡çš„ä¿¡æ¯æŸå¤±ã€‚å°†ä¸°å¯Œçš„é«˜ç»´ BERT è¯­ä¹‰å¼ºè¡Œå‹ç¼©ä¸ºå‡ ä¸ªç¦»æ•£çš„èšç±»ä¸­å¿ƒï¼Œä¸¢å¤±äº†æ–°é—»æ ‡é¢˜ä¸­å¾®å¦™çš„è¯­ä¹‰å·®å¼‚ï¼ˆFine-grained Semanticsï¼‰ã€‚\n",
    "\n",
    "#### æ”¹è¿›å’Œåˆ›æ–°ï¼š\n",
    "é’ˆå¯¹ CTR é¢„ä¼°è¿™ç§å¯¹ç²¾åº¦è¦æ±‚æé«˜çš„æ’åºï¼ˆRankingï¼‰ä»»åŠ¡ï¼Œæˆ‘è°ƒæ•´äº†ç­–ç•¥ã€‚æˆ‘ä¿ç•™äº† COBRA çš„æ ¸å¿ƒæ€æƒ³â€”â€”â€˜å¼•å…¥å¤–éƒ¨è¯­ä¹‰çŸ¥è¯†â€™ï¼Œä½†æ‘’å¼ƒäº†å®ƒçš„â€˜ç¦»æ•£åŒ–â€™è¿‡ç¨‹ã€‚ æˆ‘æ”¹ç”¨ BERT + PCA é™ç»´ + Embedding Warm-up çš„æ–¹æ¡ˆã€‚è¿™ç§**â€˜è¿ç»­è¯­ä¹‰æ³¨å…¥â€™**ç­–ç•¥ï¼Œæ—¢ä¿ç•™äº†é¢„è®­ç»ƒæ¨¡å‹çš„é«˜ç»´è¯­ä¹‰ç©ºé—´ç»“æ„ï¼Œåˆé¿å…äº†é‡åŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§æ”¹è¿›æ–¹æ¡ˆå°† AUC æå‡åˆ°äº† 0.71ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆ RQ-KMeans åœ¨ä½ çš„å®éªŒä¸­æ•ˆæœä¸å¥½\n",
    "COBRA/RQ-VAE çš„ä¸»åœºï¼šé€šå¸¸ç”¨äº**å¬å›ï¼ˆRetrievalï¼‰**é˜¶æ®µã€‚å¬å›éœ€è¦åœ¨ç™¾ä¸‡çº§æ•°æ®é‡Œå¿«é€Ÿæå‡º 100 ä¸ªå€™é€‰ï¼Œæ‰€ä»¥éœ€è¦ç¦»æ•£ç´¢å¼•ï¼ˆIndexableï¼‰æ¥åŠ é€Ÿã€‚è¿™æ—¶å€™ç‰ºç‰²ä¸€ç‚¹ç²¾åº¦æ¢å–é€Ÿåº¦æ˜¯å€¼å¾—çš„ã€‚\n",
    "\n",
    "DeepFM/DIN æ˜¯**ç²¾æ’ï¼ˆRankingï¼‰**æ¨¡å‹ã€‚å®ƒçš„ä»»åŠ¡æ˜¯ç»™è¿™ 100 ä¸ªå€™é€‰æ‰“åˆ†ï¼Œå†³å®šè°æ’ç¬¬ä¸€ã€‚æ’åºå¯¹ç²¾åº¦æå…¶æ•æ„Ÿã€‚\n",
    "\n",
    "K-Means æŠŠâ€œç§‘æ¯”â€å’Œâ€œè©¹å§†æ–¯â€éƒ½èšç±»ä¸ºâ€œID_5ï¼ˆç¯®çƒçƒæ˜Ÿï¼‰â€ï¼Œè¿™å¯¹å¬å›å¤Ÿç”¨äº†ï¼Œä½†å¯¹æ’åºæ¥è¯´ï¼Œè¿™ç§ç²’åº¦å¤ªç²—ï¼ˆCoarse-grainedï¼‰ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ†ä¸¤è€…ç»†å¾®çš„å·®åˆ«ï¼Œå¯¼è‡´ AUC ä¸Šä¸å»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0464b6-5bd6-48ab-af4b-e67700bd8f52",
   "metadata": {},
   "source": [
    "## é—®é¢˜ä¸‰ï¼šä½ çš„æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†å†·å¯åŠ¨é—®é¢˜çš„ï¼Ÿ\n",
    "â€œæˆ‘çš„æ¨¡å‹ä¸»è¦é€šè¿‡**åŸºäºå†…å®¹çš„è¯­ä¹‰è¿ç§»ï¼ˆContent-Based Semantic Transferï¼‰å’Œå±‚çº§ç‰¹å¾æ³›åŒ–ï¼ˆHierarchical Feature Generalizationï¼‰**æ¥è§£å†³å†·å¯åŠ¨é—®é¢˜ã€‚â€\n",
    "\n",
    "### 1. åˆ©ç”¨ BERT å®ç°â€œè¯­ä¹‰è¿ç§»â€\n",
    "å¯¹äºæ²¡æœ‰äº¤äº’è¡Œä¸ºçš„æ–°é—»ï¼ˆItem Cold Startï¼‰ï¼Œæˆ‘åˆ©ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹æå–æ ‡é¢˜çš„è¯­ä¹‰å‘é‡ä½œä¸º Embedding çš„åˆå§‹åŒ–æƒé‡ã€‚è¿™ä½¿å¾—æ–°ç‰©å“åœ¨è¿›å…¥æ¨¡å‹çš„ç¬¬ä¸€åˆ»ï¼Œå°±æ‹¥æœ‰äº†å…·å¤‡å®é™…è¯­ä¹‰çš„å‘é‡è¡¨ç¤ºï¼Œè€Œä¸æ˜¯éšæœºå™ªå£°ã€‚æ¨¡å‹èƒ½ç›´æ¥æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œå°†å…¶æ¨èç»™å–œæ¬¢åŒç±»å†…å®¹çš„ç”¨æˆ·\n",
    "\n",
    "### 2. åˆ©ç”¨ç±»åˆ«ç‰¹å¾å®ç°â€œå±‚çº§æ³›åŒ–â€\n",
    "ID æ˜¯å…·ä½“çš„ï¼Œä½†ç±»åˆ«æ˜¯é€šç”¨çš„ã€‚æ–°æ–°é—» N99999 è™½ç„¶æ²¡è§è¿‡ï¼Œä½†å®ƒçš„ç±»åˆ« Sports å’Œå­ç±»åˆ« Basketball æ˜¯æ¨¡å‹å·²ç»åœ¨è®­ç»ƒé›†ä¸­å­¦å¾—å¾ˆå¥½çš„ã€‚ ä½ çš„åšæ³•ï¼š ä½ çš„æ¨¡å‹è¾“å…¥ä¸­æ˜¾å¼åŒ…å«äº† cat_idx (ç±»åˆ«) å’Œ subcat_idx (å­ç±»åˆ«)ã€‚â€œé™¤äº† ID ç‰¹å¾ï¼Œæˆ‘è¿˜å¼•å…¥äº† Category å’Œ SubCategory ç‰¹å¾ã€‚å½“æ–°ç‰©å“ ID ç¼ºä¹ååŒè¿‡æ»¤ä¿¡å·æ—¶ï¼ŒDIN æ¨¡å‹å¯ä»¥é€šè¿‡ç”¨æˆ·å¯¹â€˜ç±»åˆ«â€™çš„å†å²åå¥½ï¼ˆAttention æœºåˆ¶ï¼‰ï¼Œé™çº§åŒ¹é…åˆ°è¯¥ç‰©å“çš„ç±»åˆ«ç‰¹å¾ä¸Šï¼Œä»è€Œä¿è¯æ¨èçš„å‡†ç¡®æ€§ã€‚â€\n",
    "\n",
    "### 3. å¤„ç†â€œç”¨æˆ·å†·å¯åŠ¨â€\n",
    "å¯¹äºæ–°ç”¨æˆ·ï¼ˆUser Cold Startï¼‰ï¼Œå› ä¸ºæ²¡æœ‰å†å²è¡Œä¸ºåºåˆ—ï¼Œæ¨¡å‹ä¼šä½¿ç”¨ Padding Token (N0) è¿›è¡Œå¡«å……ã€‚æ­¤æ—¶ DIN çš„ Attention æœºåˆ¶å¤±æ•ˆï¼Œæ¨¡å‹ä¼šé€€åŒ–ä¸ºä¾æ®å…¨å±€åå·®ï¼ˆBiasï¼‰å’Œç‰©å“çš„åŸºç¡€ç‰¹å¾è¿›è¡Œæ¨èï¼Œè¿™å®é™…ä¸Šç±»ä¼¼äºå‘æ–°ç”¨æˆ·æ¨èç³»ç»Ÿä¸­çš„çƒ­é—¨å†…å®¹ï¼ˆPopularity-basedï¼‰ï¼Œä¿è¯äº†åŸºç¡€çš„å…œåº•ä½“éªŒã€‚\n",
    "\n",
    "ç›®å‰å¯¹äºå®Œå…¨æ— è®°å½•çš„æ–°ç”¨æˆ·ï¼Œæˆ‘æ˜¯ç”¨ Padding å¤„ç†çš„ã€‚æœªæ¥å¦‚æœä¸Šçº¿ï¼Œæˆ‘ä¼šè€ƒè™‘åœ¨è¿™ä¸ªé˜¶æ®µåŠ å…¥åŸºäºè§„åˆ™çš„çƒ­é—¨æ¨èæˆ–è€…åŸºäºç”¨æˆ·æ³¨å†Œç”»åƒï¼ˆå¹´é¾„ã€æ€§åˆ«ï¼‰çš„ç²—ç²’åº¦æ¨èæ¥è¿›ä¸€æ­¥ä¼˜åŒ–ç”¨æˆ·å†·å¯åŠ¨ä½“éªŒã€‚\n",
    "\n",
    "## é—®é¢˜å››ï¼šä½ çš„æ–¹æ³•æ¯”ä¼ ç»Ÿçš„ DeepFM æˆ– W&D å¥½åœ¨å“ªé‡Œï¼Ÿ\n",
    "ä¼ ç»Ÿçš„æ·±åº¦æ¨èæ¨¡å‹é€šå¸¸ä¾èµ– ID Embedding çš„éšæœºåˆå§‹åŒ–ï¼Œæ–°ç‰©å“å¿…é¡»ç§¯ç´¯å¤§é‡ç‚¹å‡»ï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰åæ‰èƒ½å­¦å¥½å‘é‡ã€‚ è€Œæˆ‘çš„æ–¹æ¡ˆç»“åˆäº† LLM (BERT)ï¼Œå®ç°äº† Zero-Shotï¼ˆé›¶æ ·æœ¬ï¼‰ æˆ– Few-Shotï¼ˆå°‘æ ·æœ¬ï¼‰ çš„å¯åŠ¨èƒ½åŠ›ã€‚æ–°ç‰©å“ä¸€ä¸Šæ¶ï¼Œæ¨¡å‹å°±çŸ¥é“å®ƒæ˜¯è®²ä»€ä¹ˆçš„ï¼Œç›´æ¥å°±èƒ½æ¨èç»™æ„Ÿå…´è¶£çš„äººï¼Œæå¤§ç¼©çŸ­äº†å†·å¯åŠ¨çš„é¢„çƒ­å‘¨æœŸã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e548c4c-7967-4a2f-814e-fdb504d0c2c0",
   "metadata": {},
   "source": [
    "#### æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–å›¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6858872c-d396-4b68-91f9-9fec7f17921a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åˆ†æå›ºå®šæ ·æœ¬ Index: 2694003 ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_din' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 143\u001b[0m\n\u001b[1;32m    139\u001b[0m target_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2694003\u001b[39m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæ­£åœ¨åˆ†æå›ºå®šæ ·æœ¬ Index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m visualize_din_attention_final(\n\u001b[0;32m--> 143\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_din\u001b[49m, \n\u001b[1;32m    144\u001b[0m     df\u001b[38;5;241m=\u001b[39mval_df, \n\u001b[1;32m    145\u001b[0m     sample_index\u001b[38;5;241m=\u001b[39mtarget_idx, \n\u001b[1;32m    146\u001b[0m     news_map\u001b[38;5;241m=\u001b[39mnews_map, \n\u001b[1;32m    147\u001b[0m     all_news_df\u001b[38;5;241m=\u001b[39mall_news\n\u001b[1;32m    148\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_din' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import platform\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "# ============================\n",
    "# å­—ä½“è®¾ç½®\n",
    "# ============================\n",
    "system_name = platform.system()\n",
    "if system_name == \"Darwin\": \n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang HK', 'Heiti TC']\n",
    "else: \n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False \n",
    "\n",
    "def visualize_din_attention_final(model, df, sample_index, news_map, all_news_df):\n",
    "    \"\"\"\n",
    "    ã€æœ€ç»ˆå®Œç¾ç‰ˆã€‘å¯è§†åŒ– DIN æ³¨æ„åŠ›æƒé‡\n",
    "    ç‰¹æ€§ï¼š\n",
    "    1. åŒ…å«æ–°é—»ç±»åˆ«æ ‡ç­¾ [Category]\n",
    "    2. ä¿®å¤æ‰€æœ‰æ˜¾ç¤ºå’Œè®¡ç®—Bug\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨\n",
    "    if sample_index not in df.index:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šç´¢å¼• {sample_index} ä¸åœ¨å½“å‰éªŒè¯é›†ä¸­ã€‚è¯·æ£€æŸ¥ç´¢å¼•å·æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "        return\n",
    "\n",
    "    # --- 1. æ•°æ®å‡†å¤‡ ---\n",
    "    row = df.loc[sample_index] # ä½¿ç”¨ loc ä»¥æ”¯æŒç‰¹å®šç´¢å¼•å·\n",
    "    target_id = int(row['news_id_idx'])\n",
    "    \n",
    "    inv_news_map = {v: k for k, v in news_map.items()}\n",
    "    target_news_raw_id = inv_news_map.get(target_id, \"UNK\")\n",
    "    \n",
    "    # è·å– Target æ–°é—»çš„è¯¦ç»†ä¿¡æ¯ (æ ‡é¢˜ + ç±»åˆ«)\n",
    "    try:\n",
    "        # æŸ¥æ‰¾å¯¹åº”çš„è¡Œ\n",
    "        tgt_info = all_news_df[all_news_df['news_id'] == target_news_raw_id].iloc[0]\n",
    "        t_cat = tgt_info['category']\n",
    "        t_sub = tgt_info['subcategory']\n",
    "        t_title = tgt_info['title']\n",
    "        # æ ¼å¼åŒ–æ ‡é¢˜ï¼šã€ä½“è‚²/ç¯®çƒã€‘æ¹–äººé˜Ÿèµ¢äº†...\n",
    "        target_display = f\"ã€{t_cat}/{t_sub}ã€‘\\n{t_title}\"\n",
    "    except:\n",
    "        target_display = f\"æœªçŸ¥æ–°é—» (ID: {target_news_raw_id})\"\n",
    "\n",
    "    # è§£æå†å²åºåˆ—\n",
    "    hist_seq = row['history_seq'] \n",
    "    if isinstance(hist_seq, str): \n",
    "        hist_seq = eval(hist_seq)\n",
    "    valid_hist_ids = [int(idx) for idx in hist_seq if idx != 0]\n",
    "    \n",
    "    if len(valid_hist_ids) == 0:\n",
    "        print(\"âš ï¸ è¯¥ç”¨æˆ·æ²¡æœ‰æœ‰æ•ˆå†å²è¡Œä¸ºã€‚\")\n",
    "        return\n",
    "\n",
    "    # è·å– History æ–°é—»çš„è¯¦ç»†ä¿¡æ¯\n",
    "    hist_labels = []\n",
    "    for idx in valid_hist_ids:\n",
    "        try:\n",
    "            raw_id = inv_news_map.get(idx, \"UNK\")\n",
    "            h_info = all_news_df[all_news_df['news_id'] == raw_id].iloc[0]\n",
    "            \n",
    "            h_cat = h_info['category']\n",
    "            h_title = h_info['title']\n",
    "            \n",
    "            # æ ¼å¼åŒ–å†å²æ ‡ç­¾: [ä½“è‚²] æ ‡é¢˜...\n",
    "            # æˆªæ–­æ ‡é¢˜é˜²æ­¢è¿‡é•¿\n",
    "            short_title = h_title[:70] + \"...\" if len(h_title) > 70 else h_title\n",
    "            label = f\"[{h_cat}] {short_title}\"\n",
    "            hist_labels.append(label)\n",
    "        except:\n",
    "            hist_labels.append(\"Unknown Item\")\n",
    "\n",
    "    # --- 2. æå–æƒé‡ ---\n",
    "    try:\n",
    "        emb_layer = model.embedding.embed_dict['news_id_idx']\n",
    "        attn_layer = model.attention_layers[0] \n",
    "    except:\n",
    "        print(\"âŒ æ¨¡å‹ç»“æ„ä¸åŒ¹é…ï¼Œæ— æ³•æå–å±‚ã€‚\")\n",
    "        return\n",
    "    \n",
    "    # --- 3. è®¡ç®—åˆ†æ•° (BatchNormä¿®å¤ç‰ˆ) ---\n",
    "    with torch.no_grad():\n",
    "        target_emb = emb_layer(torch.tensor([target_id]).to(device)) \n",
    "        keys_emb = emb_layer(torch.tensor([valid_hist_ids]).to(device)) \n",
    "        query_emb = target_emb.expand(keys_emb.shape[0], keys_emb.shape[1], -1)\n",
    "        \n",
    "        din_input = torch.cat([query_emb, keys_emb, query_emb - keys_emb, query_emb * keys_emb], dim=-1)\n",
    "        \n",
    "        # Flatten\n",
    "        seq_len = din_input.shape[1]\n",
    "        dim = din_input.shape[2]\n",
    "        din_input_flat = din_input.view(-1, dim) \n",
    "        \n",
    "        scores_flat = attn_layer.attention(din_input_flat) \n",
    "        attention_weights = scores_flat.squeeze().cpu().numpy()\n",
    "        \n",
    "        if attention_weights.ndim == 0:\n",
    "            attention_weights = np.array([attention_weights])\n",
    "\n",
    "    # --- 4. ç»˜å›¾ ---\n",
    "    # é«˜åº¦æ ¹æ®å†å²è®°å½•æ•°é‡è‡ªåŠ¨è°ƒæ•´\n",
    "    fig, ax = plt.subplots(figsize=(13, len(valid_hist_ids) * 0.7 + 2))\n",
    "    \n",
    "    w_min, w_max = attention_weights.min(), attention_weights.max()\n",
    "    norm_weights = (attention_weights - w_min) / (w_max - w_min + 1e-9)\n",
    "    colors = plt.cm.coolwarm(norm_weights)\n",
    "    \n",
    "    y_pos = range(len(valid_hist_ids))\n",
    "    ax.barh(y_pos, attention_weights, color=colors, height=0.6)\n",
    "    \n",
    "    # è®¾ç½®å·¦ä¾§æ ‡ç­¾\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(hist_labels, fontsize=11, family='Arial Unicode MS') # å¼ºåˆ¶æŒ‡å®šæ”¯æŒä¸­æ–‡çš„å­—ä½“\n",
    "    \n",
    "    ax.set_xlabel('Attention Score (å…³è”åº¦)', fontsize=12)\n",
    "    # æ ‡é¢˜å·¦å¯¹é½æ˜¾ç¤º\n",
    "    ax.set_title(f'DIN æ¨¡å‹æ³¨æ„åŠ›é€è§†\\nå€™é€‰ç›®æ ‡: {target_display}', fontsize=14, color='darkblue', fontweight='bold', loc='left', pad=20)\n",
    "    \n",
    "    for i, v in enumerate(attention_weights):\n",
    "        ax.text(v, i, f\" {v:.2f}\", va='center', fontsize=10, fontweight='bold')\n",
    "        \n",
    "    # è°ƒæ•´è¾¹è·ï¼šå·¦è¾¹ç•™å‡º 40% ç»™å¸¦ç±»åˆ«çš„é•¿æ ‡é¢˜\n",
    "    plt.subplots_adjust(left=0.4, right=0.95, top=0.85, bottom=0.1)\n",
    "    \n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# ================================\n",
    "# ğŸš€ è¿è¡Œ (å›ºå®š Index: 2694003)\n",
    "# ================================\n",
    "target_idx = 2694003\n",
    "\n",
    "print(f\"æ­£åœ¨åˆ†æå›ºå®šæ ·æœ¬ Index: {target_idx} ...\")\n",
    "visualize_din_attention_final(\n",
    "    model=model_din, \n",
    "    df=val_df, \n",
    "    sample_index=target_idx, \n",
    "    news_map=news_map, \n",
    "    all_news_df=all_news\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e84e78-e940-401b-9406-9dad011aed1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40c0d57b-5b91-46fe-b55a-2bcef0a69991",
   "metadata": {},
   "source": [
    "#### ä¸Šä¼ ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674074-5733-4524-9b90-389a22fa447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼ºåˆ¶å¯¹é½æ¨¡å‹éª¨æ¶: User=750434, News=101288\n",
      "æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹...\n",
      "âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼(éª¨æ¶å·²å¼ºè¡Œé€‚é…)\n",
      "è¯»å–æµ‹è¯•é›†åŸå§‹æ–‡ä»¶...\n",
      "æ„å»ºæµ‹è¯•é›†æ ·æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2370727/2370727 [01:45<00:00, 22543.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‰¹å¾æ˜ å°„ä¸­ (å¯ç”¨å®‰å…¨æ¨¡å¼)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import rankdata\n",
    "from torch_rechub.models.ranking import DIN\n",
    "from torch_rechub.basic.features import SparseFeature, SequenceFeature\n",
    "from torch_rechub.utils.data import DataGenerator\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ==========================================\n",
    "# 1. å¼ºåˆ¶å¯¹é½é…ç½® (æ ¹æ®ä½ çš„æŠ¥é”™ä¿¡æ¯ç¡¬ç¼–ç )\n",
    "# ==========================================\n",
    "EMBEDDING_DIM = 32\n",
    "device = 'cpu' \n",
    "\n",
    "# âš ï¸ å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨æŠ¥é”™ä¿¡æ¯é‡Œ \"checkpoint\" çš„æ•°å€¼\n",
    "# User: copying a param with shape torch.Size([750434, 32])\n",
    "FIXED_USER_VOCAB = 750434 \n",
    "\n",
    "# News: copying a param with shape torch.Size([101288, 32])\n",
    "FIXED_NEWS_VOCAB = 101288 \n",
    "\n",
    "# Cat/Subcat æ²¡æŠ¥é”™ï¼Œè¯´æ˜ç»´åº¦æ²¡å˜ï¼Œæ²¿ç”¨ä¹‹å‰è®¡ç®—çš„ (å‡è®¾ä¹‹å‰ç¯å¢ƒæ¢å¤ä»£ç å·²è·‘)\n",
    "# å¦‚æœ cat_idx ä¹Ÿæ²¡å¯¹é½ï¼Œéœ€è¦æŠŠ vocab_size_cat ä¹Ÿç¡¬ç¼–ç ï¼Œä½†é€šå¸¸è¿™ä¸ªæ¯”è¾ƒç¨³\n",
    "if 'vocab_size_cat' not in globals():\n",
    "    # ä¸‡ä¸€å†…å­˜ç©ºäº†ï¼Œç»™ä¸ªä¿åº•å€¼ (MINDæ•°æ®é›†é€šå¸¸ Cat<20, Subcat<300)\n",
    "    vocab_size_cat = 30\n",
    "    vocab_size_subcat = 300\n",
    "\n",
    "print(f\"ğŸš€ å¼ºåˆ¶å¯¹é½æ¨¡å‹éª¨æ¶: User={FIXED_USER_VOCAB}, News={FIXED_NEWS_VOCAB}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. é‡æ–°å®šä¹‰æ¨¡å‹éª¨æ¶ (Features)\n",
    "# ==========================================\n",
    "# User ç‰¹å¾\n",
    "user_cols = [SparseFeature(\"user_id_idx\", vocab_size=FIXED_USER_VOCAB, embed_dim=EMBEDDING_DIM)]\n",
    "\n",
    "# Item ç‰¹å¾\n",
    "item_cols = [\n",
    "    SparseFeature(\"news_id_idx\", vocab_size=FIXED_NEWS_VOCAB, embed_dim=EMBEDDING_DIM),\n",
    "    SparseFeature(\"cat_idx\", vocab_size=vocab_size_cat, embed_dim=EMBEDDING_DIM),        \n",
    "    SparseFeature(\"subcat_idx\", vocab_size=vocab_size_subcat, embed_dim=EMBEDDING_DIM)\n",
    "]\n",
    "\n",
    "# History ç‰¹å¾\n",
    "history_cols = [\n",
    "    SequenceFeature(\"history_seq\", vocab_size=FIXED_NEWS_VOCAB, embed_dim=EMBEDDING_DIM, pooling=\"concat\", shared_with=\"news_id_idx\"),\n",
    "    SequenceFeature(\"hist_cat_seq\", vocab_size=vocab_size_cat, embed_dim=EMBEDDING_DIM, pooling=\"concat\", shared_with=\"cat_idx\"),\n",
    "    SequenceFeature(\"hist_subcat_seq\", vocab_size=vocab_size_subcat, embed_dim=EMBEDDING_DIM, pooling=\"concat\", shared_with=\"subcat_idx\")\n",
    "]\n",
    "\n",
    "features = user_cols + item_cols\n",
    "\n",
    "# ==========================================\n",
    "# 3. åŠ è½½æ¨¡å‹\n",
    "# ==========================================\n",
    "print(\"æ­£åœ¨åŠ è½½æœ€ä½³æ¨¡å‹...\")\n",
    "model_din = DIN(\n",
    "    features=features,\n",
    "    history_features=history_cols,\n",
    "    target_features=item_cols,\n",
    "    mlp_params={\"dims\": [256, 128], \"dropout\": 0.3},\n",
    "    attention_mlp_params={\"dims\": [64, 32]} \n",
    ")\n",
    "\n",
    "try:\n",
    "    model_din.load_state_dict(torch.load('din_best_model.pth', map_location=device))\n",
    "    model_din.to(device)\n",
    "    model_din.eval()\n",
    "    print(\"âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸï¼(éª¨æ¶å·²å¼ºè¡Œé€‚é…)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"âŒ ä¾ç„¶å¤±è´¥: {e}\")\n",
    "    raise\n",
    "\n",
    "# ==========================================\n",
    "# 4. å¤„ç†æµ‹è¯•é›† (å¸¦è¶Šç•Œä¿æŠ¤)\n",
    "# ==========================================\n",
    "# å®šä¹‰å¤„ç†å‡½æ•°\n",
    "def process_test_data(df):\n",
    "    samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing Test Set\"):\n",
    "        user_id = str(row['user_id'])\n",
    "        imp_id = str(row['impression_id'])\n",
    "        hist = str(row['history']).split() if pd.notna(row['history']) else ['N0']\n",
    "        hist_str = \" \".join(hist)\n",
    "        impressions = str(row['impressions']).split()\n",
    "        for i, news_id in enumerate(impressions):\n",
    "            samples.append([user_id, hist_str, news_id, imp_id, i])\n",
    "    return pd.DataFrame(samples, columns=['user_id', 'history_str', 'news_id', 'impression_id', 'imp_index'])\n",
    "\n",
    "# è¯»å–æµ‹è¯•é›†\n",
    "if 'test_behaviors' not in globals():\n",
    "    print(\"è¯»å–æµ‹è¯•é›†åŸå§‹æ–‡ä»¶...\")\n",
    "    try:\n",
    "        test_behaviors = pd.read_csv('data/MINDlarge_test/behaviors.tsv', sep='\\t', header=None, names=behaviors_cols)\n",
    "    except:\n",
    "        # å°è¯•å¤‡ç”¨è·¯å¾„\n",
    "        test_behaviors = pd.read_csv('data/MINDlarge_test/behaviors.tsv', sep='\\t', header=None, names=['impression_id', 'user_id', 'time', 'history', 'impressions'])\n",
    "\n",
    "print(\"æ„å»ºæµ‹è¯•é›†æ ·æœ¬...\")\n",
    "test_df = process_test_data(test_behaviors)\n",
    "\n",
    "# --- ç‰¹å¾æ˜ å°„ä¸è¶Šç•Œæ¸…æ´— ---\n",
    "print(\"ç‰¹å¾æ˜ å°„ä¸­ (å¯ç”¨å®‰å…¨æ¨¡å¼)...\")\n",
    "\n",
    "# User æ˜ å°„\n",
    "# å³ä½¿å†…å­˜é‡Œæœ‰ lbe_userï¼Œå®ƒçš„æ˜ å°„å…³ç³»å¯èƒ½å’Œè®­ç»ƒæ—¶ä¸å®Œå…¨ä¸€è‡´ï¼Œä½† ID é¡ºåºé€šå¸¸ä¸å˜\n",
    "# æˆ‘ä»¬å‡è®¾ lbe_user æ˜¯å­˜åœ¨çš„ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œè¿™é‡Œä¼šæŠ¥é”™ï¼Œä½ éœ€è¦å…ˆè·‘ä¸Šé¢çš„æ¢å¤ä»£ç \n",
    "user_map = dict(zip(lbe_user.classes_, range(len(lbe_user.classes_))))\n",
    "test_df['user_id_idx'] = test_df['user_id'].map(user_map).fillna(0).astype(int)\n",
    "\n",
    "# ã€è¶Šç•Œä¿æŠ¤ã€‘å¦‚æœç°åœ¨çš„ ID è¶…è¿‡äº†æ¨¡å‹æ‰¿å—èŒƒå›´ (750434)ï¼Œå¼ºåˆ¶è®¾ä¸º 0 (UNK)\n",
    "test_df['user_id_idx'] = test_df['user_id_idx'].apply(lambda x: x if x < FIXED_USER_VOCAB else 0)\n",
    "\n",
    "# News ID æ˜ å°„\n",
    "test_df['news_id_idx'] = test_df['news_id'].apply(lambda x: news_map.get(x, 0))\n",
    "# ã€è¶Šç•Œä¿æŠ¤ã€‘\n",
    "test_df['news_id_idx'] = test_df['news_id_idx'].apply(lambda x: x if x < FIXED_NEWS_VOCAB else 0)\n",
    "\n",
    "# Cat / SubCat\n",
    "test_df['cat_idx'] = test_df['news_id'].apply(lambda x: news2cat.get(x, 0))\n",
    "test_df['subcat_idx'] = test_df['news_id'].apply(lambda x: news2subcat.get(x, 0))\n",
    "\n",
    "# å†å²åºåˆ—æ˜ å°„\n",
    "def get_seq_ids_safe(s, mapping, max_vocab):\n",
    "    # æ˜ å°„å¹¶è¿›è¡Œå®‰å…¨æ£€æŸ¥\n",
    "    ids = [mapping.get(x, 0) for x in s.split()]\n",
    "    return [i if i < max_vocab else 0 for i in ids]\n",
    "\n",
    "test_df['hist_idx_list'] = test_df['history_str'].apply(lambda x: get_seq_ids_safe(x, news_map, FIXED_NEWS_VOCAB))\n",
    "test_df['hist_cat_list'] = test_df['history_str'].apply(lambda x: get_seq_ids_safe(x, news2cat, 999999)) # Caté€šå¸¸ä¸ä¼šè¶Šç•Œ\n",
    "test_df['hist_subcat_list'] = test_df['history_str'].apply(lambda x: get_seq_ids_safe(x, news2subcat, 999999))\n",
    "\n",
    "# Padding\n",
    "print(\"Paddingåºåˆ—...\")\n",
    "SEQ_LEN = 20\n",
    "def pad(series):\n",
    "    return list(pad_sequences(series.tolist(), maxlen=SEQ_LEN, padding='post', truncating='post', value=0))\n",
    "\n",
    "test_df['history_seq'] = pad(test_df['hist_idx_list'])\n",
    "test_df['hist_cat_seq'] = pad(test_df['hist_cat_list'])\n",
    "test_df['hist_subcat_seq'] = pad(test_df['hist_subcat_list'])\n",
    "\n",
    "# ==========================================\n",
    "# 5. é¢„æµ‹å¹¶æ‰“åŒ…\n",
    "# ==========================================\n",
    "input_cols = ['user_id_idx', 'news_id_idx', 'cat_idx', 'subcat_idx', \n",
    "              'history_seq', 'hist_cat_seq', 'hist_subcat_seq']\n",
    "\n",
    "test_dg = DataGenerator(x=test_df[input_cols], y=test_df['impression_id'])\n",
    "test_loader, _, _ = test_dg.generate_dataloader(x_val=test_df[input_cols], y_val=test_df['impression_id'], batch_size=4096, shuffle=False)\n",
    "\n",
    "print(\"æ­£åœ¨é¢„æµ‹...\")\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        x = {k: v.to(device) for k, v in x.items()}\n",
    "        pred = model_din(x).cpu().data.numpy().squeeze()\n",
    "        y_pred.extend(pred)\n",
    "\n",
    "test_df['score'] = y_pred\n",
    "\n",
    "print(\"ç”Ÿæˆ prediction.txt ...\")\n",
    "lines = []\n",
    "for imp_id, group in tqdm(test_df.groupby('impression_id'), desc=\"Grouping\"):\n",
    "    group = group.sort_values('imp_index')\n",
    "    scores = group['score'].values\n",
    "    ranks = rankdata(-scores, method='ordinal').astype(int)\n",
    "    rank_str = '[' + ','.join(map(str, ranks)) + ']'\n",
    "    lines.append(f\"{imp_id} {rank_str}\")\n",
    "\n",
    "with open('prediction.txt', 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(\"æ­£åœ¨å‹ç¼©...\")\n",
    "with zipfile.ZipFile('prediction.zip', 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write('prediction.txt')\n",
    "\n",
    "print(\"ğŸ‰ æ­å–œï¼é—®é¢˜è§£å†³ï¼Œprediction.zip å·²ç”Ÿæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1624dff-2126-4136-9780-d141d6376ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ­£åœ¨æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: din_best_model.pth ...\n",
      "========================================\n",
      "âœ… æ£€æµ‹ç»“æœå‡ºç‚‰ï¼\n",
      "æƒé‡åç§°: embedding.embed_dict.news_id_idx.weight\n",
      "æƒé‡å½¢çŠ¶: torch.Size([101288, 32])\n",
      "ğŸ‘‰ ä½ çš„æ¨¡å‹çœŸå®ç»´åº¦ (EMBEDDING_DIM) æ˜¯: ã€32ã€‘\n",
      "========================================\n",
      "è¯·åœ¨åç»­ä»£ç ä¸­è®¾ç½®: EMBEDDING_DIM = 32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = 'din_best_model.pth'\n",
    "\n",
    "print(f\"ğŸ” æ­£åœ¨æ£€æŸ¥æ¨¡å‹æ–‡ä»¶: {model_path} ...\")\n",
    "\n",
    "try:\n",
    "    # åŠ è½½æ¨¡å‹å‚æ•°å­—å…¸ (æ˜ å°„åˆ° CPU è¯»å–)\n",
    "    state_dict = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # æŸ¥æ‰¾ news_id_idx çš„æƒé‡\n",
    "    # é€šå¸¸ key æ˜¯ 'embedding.embed_dict.news_id_idx.weight'\n",
    "    target_key = None\n",
    "    for key in state_dict.keys():\n",
    "        if 'news_id_idx' in key and 'weight' in key:\n",
    "            target_key = key\n",
    "            break\n",
    "            \n",
    "    if target_key:\n",
    "        weight_shape = state_dict[target_key].shape\n",
    "        actual_dim = weight_shape[1] # [vocab_size, embedding_dim]\n",
    "        \n",
    "        print(\"=\"*40)\n",
    "        print(f\"âœ… æ£€æµ‹ç»“æœå‡ºç‚‰ï¼\")\n",
    "        print(f\"æƒé‡åç§°: {target_key}\")\n",
    "        print(f\"æƒé‡å½¢çŠ¶: {weight_shape}\")\n",
    "        print(f\"ğŸ‘‰ ä½ çš„æ¨¡å‹çœŸå®ç»´åº¦ (EMBEDDING_DIM) æ˜¯: ã€{actual_dim}ã€‘\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # è‡ªåŠ¨ç”Ÿæˆå»ºè®®\n",
    "        if actual_dim == 32:\n",
    "            print(\"è¯·åœ¨åç»­ä»£ç ä¸­è®¾ç½®: EMBEDDING_DIM = 32\")\n",
    "        elif actual_dim == 64:\n",
    "            print(\"è¯·åœ¨åç»­ä»£ç ä¸­è®¾ç½®: EMBEDDING_DIM = 64\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ° News ID çš„ Embedding å±‚ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´ã€‚\")\n",
    "        # æ‰“å°æ‰€æœ‰ keys å¸®ä½ æ’æŸ¥\n",
    "        print(\"æ‰€æœ‰ Keys:\", state_dict.keys())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {model_path}ã€‚è¯·ç¡®è®¤æ–‡ä»¶åæ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¯»å–å‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c73c9e-d18f-4dab-b68b-eadd52705fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125c0e8-0856-4ee1-b213-a23fc746fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
